{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Ingest the data we have gathered into a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>size_of_image</th>\n",
       "      <th>size_of_code</th>\n",
       "      <th>size_of_stack_reserve</th>\n",
       "      <th>size_of_stack_commit</th>\n",
       "      <th>size_of_heap_reserve</th>\n",
       "      <th>size_of_heap_commit</th>\n",
       "      <th>size_dif</th>\n",
       "      <th>stack_dif</th>\n",
       "      <th>heap_dif</th>\n",
       "      <th>file_entropy</th>\n",
       "      <th>number_of_sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>9.289800e+04</td>\n",
       "      <td>92898.000000</td>\n",
       "      <td>92898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.252095e+05</td>\n",
       "      <td>8.138984e+05</td>\n",
       "      <td>1.581340e+06</td>\n",
       "      <td>1.011728e+06</td>\n",
       "      <td>1.762707e+04</td>\n",
       "      <td>1.064151e+06</td>\n",
       "      <td>1.933732e+04</td>\n",
       "      <td>1.886889e+05</td>\n",
       "      <td>9.941011e+05</td>\n",
       "      <td>1.044814e+06</td>\n",
       "      <td>6.396096</td>\n",
       "      <td>4.720457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.740078e+06</td>\n",
       "      <td>5.962810e+06</td>\n",
       "      <td>4.839191e+07</td>\n",
       "      <td>3.152560e+06</td>\n",
       "      <td>3.145334e+06</td>\n",
       "      <td>6.397040e+06</td>\n",
       "      <td>3.969503e+06</td>\n",
       "      <td>5.790411e+06</td>\n",
       "      <td>9.616575e+05</td>\n",
       "      <td>2.452672e+06</td>\n",
       "      <td>1.121927</td>\n",
       "      <td>2.049490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.886902e+08</td>\n",
       "      <td>-4.123059e+07</td>\n",
       "      <td>-1.579827e+07</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.396050e+04</td>\n",
       "      <td>8.192000e+04</td>\n",
       "      <td>2.304000e+04</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>4.410000e+03</td>\n",
       "      <td>1.032192e+06</td>\n",
       "      <td>1.044480e+06</td>\n",
       "      <td>5.917650</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.466315e+05</td>\n",
       "      <td>2.293760e+05</td>\n",
       "      <td>7.782400e+04</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>1.484800e+04</td>\n",
       "      <td>1.044480e+06</td>\n",
       "      <td>1.044480e+06</td>\n",
       "      <td>6.453194</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.374400e+05</td>\n",
       "      <td>6.389760e+05</td>\n",
       "      <td>2.621440e+05</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>4.096000e+03</td>\n",
       "      <td>2.573400e+04</td>\n",
       "      <td>1.044480e+06</td>\n",
       "      <td>1.044480e+06</td>\n",
       "      <td>7.216248</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.013673e+08</td>\n",
       "      <td>1.108480e+09</td>\n",
       "      <td>4.294967e+09</td>\n",
       "      <td>9.171352e+08</td>\n",
       "      <td>9.583658e+08</td>\n",
       "      <td>1.948318e+09</td>\n",
       "      <td>1.208490e+09</td>\n",
       "      <td>1.107230e+09</td>\n",
       "      <td>3.355034e+07</td>\n",
       "      <td>7.398284e+08</td>\n",
       "      <td>7.999939</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_size  size_of_image  size_of_code  size_of_stack_reserve  \\\n",
       "count  9.289800e+04   9.289800e+04  9.289800e+04           9.289800e+04   \n",
       "mean   6.252095e+05   8.138984e+05  1.581340e+06           1.011728e+06   \n",
       "std    1.740078e+06   5.962810e+06  4.839191e+07           3.152560e+06   \n",
       "min    5.120000e+02   0.000000e+00  0.000000e+00           0.000000e+00   \n",
       "25%    7.396050e+04   8.192000e+04  2.304000e+04           1.048576e+06   \n",
       "50%    2.466315e+05   2.293760e+05  7.782400e+04           1.048576e+06   \n",
       "75%    6.374400e+05   6.389760e+05  2.621440e+05           1.048576e+06   \n",
       "max    2.013673e+08   1.108480e+09  4.294967e+09           9.171352e+08   \n",
       "\n",
       "       size_of_stack_commit  size_of_heap_reserve  size_of_heap_commit  \\\n",
       "count          9.289800e+04          9.289800e+04         9.289800e+04   \n",
       "mean           1.762707e+04          1.064151e+06         1.933732e+04   \n",
       "std            3.145334e+06          6.397040e+06         3.969503e+06   \n",
       "min            0.000000e+00          0.000000e+00         0.000000e+00   \n",
       "25%            4.096000e+03          1.048576e+06         4.096000e+03   \n",
       "50%            4.096000e+03          1.048576e+06         4.096000e+03   \n",
       "75%            4.096000e+03          1.048576e+06         4.096000e+03   \n",
       "max            9.583658e+08          1.948318e+09         1.208490e+09   \n",
       "\n",
       "           size_dif     stack_dif      heap_dif  file_entropy  \\\n",
       "count  9.289800e+04  9.289800e+04  9.289800e+04  92898.000000   \n",
       "mean   1.886889e+05  9.941011e+05  1.044814e+06      6.396096   \n",
       "std    5.790411e+06  9.616575e+05  2.452672e+06      1.121927   \n",
       "min   -1.886902e+08 -4.123059e+07 -1.579827e+07      0.036711   \n",
       "25%    4.410000e+03  1.032192e+06  1.044480e+06      5.917650   \n",
       "50%    1.484800e+04  1.044480e+06  1.044480e+06      6.453194   \n",
       "75%    2.573400e+04  1.044480e+06  1.044480e+06      7.216248   \n",
       "max    1.107230e+09  3.355034e+07  7.398284e+08      7.999939   \n",
       "\n",
       "       number_of_sections  \n",
       "count        92898.000000  \n",
       "mean             4.720457  \n",
       "std              2.049490  \n",
       "min              0.000000  \n",
       "25%              3.000000  \n",
       "50%              5.000000  \n",
       "75%              6.000000  \n",
       "max             72.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('D:\\\\malware\\\\output.csv')\n",
    "\n",
    "df['size_dif'] = df['size_of_image'] - df['file_size']\n",
    "df['stack_dif'] = df['size_of_stack_reserve'] - df['size_of_stack_commit']\n",
    "df['heap_dif'] = df['size_of_heap_reserve'] - df['size_of_heap_commit']\n",
    "\n",
    "df['imports'] = df['imports'].apply(lambda i: ' '.join(i.strip('[]').replace(\"'\", \"\").replace(\" \", \"\").split(',')))\n",
    "\n",
    "normalize_candidates = [\n",
    "    'file_size', 'size_of_image', 'size_of_code', 'size_of_stack_reserve',\n",
    "    'size_of_stack_commit', 'size_of_heap_reserve', 'size_of_heap_commit',\n",
    "    'size_dif', 'stack_dif', 'heap_dif', 'file_entropy', 'number_of_sections' \n",
    "]\n",
    "\n",
    "df[normalize_candidates].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and testing. \n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, valid = train_test_split(train, test_size=0.2)\n",
    "\n",
    "#imp_ds = tf.data.Dataset.from_tensor_slices(df['imports'])\n",
    "#type_ds = tf.data.Dataset.from_tensor_slices(df['file_type'])\n",
    "\n",
    "def Normalize(df):\n",
    "    return (df-df.min())/(df.max()-df.min())\n",
    "\n",
    "for cand in normalize_candidates:\n",
    "    df[cand] = Normalize(df[cand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model(learning_rate, feature_layer, metrics):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(feature_layer)\n",
    "    model.add(layers.Dense(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        kernel_regularizer='l2',\n",
    "        name='Hidden_1'\n",
    "    ))\n",
    "    model.add(layers.Dense(\n",
    "        32,\n",
    "        activation='relu',\n",
    "        kernel_regularizer='l2',\n",
    "        name='Hidden_2'\n",
    "    ))\n",
    "    model.add(layers.Dropout(0.1, name='Dropout'))\n",
    "\n",
    "    \n",
    "    model.add(layers.Dense(\n",
    "        1,\n",
    "        activation='sigmoid',\n",
    "        name='Sigmoid'\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(\n",
    "    model, dataset, epochs,\n",
    "    label_name, valid,\n",
    "    batch_size=None, \n",
    "    shuffle=True\n",
    "):\n",
    "    '''Feed dataset and label, then train model. '''\n",
    "\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name))\n",
    "    \n",
    "    validation_set = {name:np.array(value) for name, value in valid.items()}\n",
    "    validation_label = np.array(validation_set.pop(label_name))\n",
    "\n",
    "    history = model.fit(\n",
    "        x=features, \n",
    "        y=label, \n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        shuffle=shuffle,\n",
    "        validation_data=(validation_set, validation_label)\n",
    "    )\n",
    "\n",
    "    epochs = history.epoch\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "def Plot_Curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "for i in normalize_candidates:\n",
    "    feature_columns.append(\n",
    "        feature_column.numeric_column(i)\n",
    "    )\n",
    "\n",
    "feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'file_size': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>, 'size_of_image': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=int64>, 'size_of_code': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=int64>, 'size_of_stack_reserve': <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=int64>, 'size_of_stack_commit': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=int64>, 'size_of_heap_reserve': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=int64>, 'size_of_heap_commit': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, 'size_dif': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'stack_dif': <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=int64>, 'heap_dif': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>, 'file_entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'number_of_sections': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int64>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'file_size': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>, 'size_of_image': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=int64>, 'size_of_code': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=int64>, 'size_of_stack_reserve': <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=int64>, 'size_of_stack_commit': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=int64>, 'size_of_heap_reserve': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=int64>, 'size_of_heap_commit': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, 'size_dif': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'stack_dif': <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=int64>, 'heap_dif': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>, 'file_entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'number_of_sections': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int64>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/3283290214.py\", line 28, in <module>\n      epochs, hist = Train_Model(model, train[normalize_candidates], epochs, label_name, valid[normalize_candidates], batch_size)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/2493786077.py\", line 15, in Train_Model\n      history = model.fit(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 864, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 1403, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/3283290214.py\", line 28, in <module>\n      epochs, hist = Train_Model(model, train[normalize_candidates], epochs, label_name, valid[normalize_candidates], batch_size)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/2493786077.py\", line 15, in Train_Model\n      history = model.fit(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 864, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 1403, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/Output/BiasAdd:0) = ] [[0][0][0]...] [y (Cast_8/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_greater_equal_1/Assert/AssertGuard/pivot_f/_23/_87]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/Output/BiasAdd:0) = ] [[0][0][0]...] [y (Cast_8/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1730]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21168/3283290214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBuild_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnormalize_candidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnormalize_candidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmetrics_to_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#, 'precision', 'recall']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21168/2493786077.py\u001b[0m in \u001b[0;36mTrain_Model\u001b[1;34m(model, dataset, epochs, label_name, valid, batch_size, shuffle)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mvalidation_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/3283290214.py\", line 28, in <module>\n      epochs, hist = Train_Model(model, train[normalize_candidates], epochs, label_name, valid[normalize_candidates], batch_size)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/2493786077.py\", line 15, in Train_Model\n      history = model.fit(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 864, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 1403, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Program Files\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Program Files\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/3283290214.py\", line 28, in <module>\n      epochs, hist = Train_Model(model, train[normalize_candidates], epochs, label_name, valid[normalize_candidates], batch_size)\n    File \"C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/2493786077.py\", line 15, in Train_Model\n      history = model.fit(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 864, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\metrics.py\", line 1403, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/Output/BiasAdd:0) = ] [[0][0][0]...] [y (Cast_8/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_greater_equal_1/Assert/AssertGuard/pivot_f/_23/_87]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/Output/BiasAdd:0) = ] [[0][0][0]...] [y (Cast_8/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1730]"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.003\n",
    "epochs = 5\n",
    "batch_size=32\n",
    "classification_threshold = 0.55\n",
    "label_name = 'label'\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', \n",
    "        threshold=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Precision(\n",
    "        name='precision',\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Recall(\n",
    "        name=\"recall\",\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Rename the column name to match the expect input.\n",
    "normalize_candidates.append('label') \n",
    "\n",
    "model = Build_Model(learning_rate, feature_layer, metrics)\n",
    "\n",
    "epochs, hist = Train_Model(model, train[normalize_candidates], epochs, label_name, valid[normalize_candidates], batch_size)\n",
    "\n",
    "metrics_to_plot = ['accuracy']#, 'precision', 'recall']\n",
    "\n",
    "Plot_Curve(epochs, hist, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in train[normalize_candidates].items()}\n",
    "label = np.array(features.pop(label_name))\n",
    "main_pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=float32, numpy=\n",
       "array([[ 0.3059429 ,  0.04854507, -0.10244771,  0.00235147,  0.01862714,\n",
       "         0.02173721, -0.00593309,  0.02614102,  0.01498474, -0.1277027 ,\n",
       "        -0.03943286,  0.32368445, -0.04943154, -0.14988844,  0.15822528,\n",
       "         0.43203717,  0.12320857, -0.0158817 ,  0.36795557,  0.28294122,\n",
       "         0.06565747, -0.00249822,  0.34604615, -0.10406474, -0.06754161,\n",
       "        -0.27265453, -0.05341744,  0.30954793, -0.08949388,  0.00560029,\n",
       "        -0.19600613, -0.04928539, -0.35421884,  0.0037315 ,  0.1883006 ,\n",
       "        -0.2942362 ,  0.29516977,  0.14750236, -0.17053317,  0.23857348,\n",
       "        -0.00611515, -0.14055108,  0.0138289 ,  0.0108629 , -0.45722714,\n",
       "        -0.02904043,  0.2584269 ,  0.02794387, -0.2345884 ,  0.23760574]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'file_type'\n",
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(\n",
    "    embedding, \n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=True,\n",
    "    name=layer_name\n",
    ")\n",
    "hub_layer(train['file_type'][:1])\n",
    "\n",
    "#vectorize_layer_type = layers.TextVectorization(\n",
    "#    output_mode='int',\n",
    "#    output_sequence_length=8\n",
    "#)\n",
    "#vectorize_layer_type.adapt(train['file_type'])\n",
    "#vectorize_layer_type(df['file_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858/1858 [==============================] - 51s 27ms/step - loss: 0.3652 - accuracy: 0.8391 - precision: 0.9018 - recall: 0.8045 - val_loss: 0.3279 - val_accuracy: 0.8674 - val_precision: 0.9014 - val_recall: 0.8602\n",
      "Epoch 2/5\n",
      "1858/1858 [==============================] - 51s 27ms/step - loss: 0.3297 - accuracy: 0.8583 - precision: 0.9021 - recall: 0.8421 - val_loss: 0.3221 - val_accuracy: 0.8668 - val_precision: 0.9027 - val_recall: 0.8575\n",
      "Epoch 3/5\n",
      "1858/1858 [==============================] - 52s 28ms/step - loss: 0.3263 - accuracy: 0.8603 - precision: 0.9019 - recall: 0.8462 - val_loss: 0.3196 - val_accuracy: 0.8677 - val_precision: 0.9018 - val_recall: 0.8602\n",
      "Epoch 4/5\n",
      "1858/1858 [==============================] - 51s 27ms/step - loss: 0.3256 - accuracy: 0.8609 - precision: 0.9017 - recall: 0.8476 - val_loss: 0.3201 - val_accuracy: 0.8677 - val_precision: 0.9018 - val_recall: 0.8602\n",
      "Epoch 5/5\n",
      "1858/1858 [==============================] - 51s 27ms/step - loss: 0.3242 - accuracy: 0.8616 - precision: 0.9021 - recall: 0.8486 - val_loss: 0.3168 - val_accuracy: 0.8679 - val_precision: 0.9019 - val_recall: 0.8607\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnOElEQVR4nO3de3xU5b3v8c9vZkIuJCBCrMiA0FOK3A0Gb7QWtShaq61btlK1RW2t9kj3qXoUu92VbT2v0/bYy7ale5e2FKxatXq6X7aHVqVqbStuifVWRZRalCCtERSIXJLJ/M4fsxLWDCvJBGYySfi+X695ZV2etdbzzMDvt5611jxj7o6IiEiuWKkrICIifZMShIiIRFKCEBGRSEoQIiISSQlCREQiJUpdgUIZMWKEjx07ttTVEBHpV55++um33b02at2ASRBjx46loaGh1NUQEelXzOz1ztbpEpOIiERSghARkUhKECIiEkkJQkREIilBiIhIJCUIERGJpAQhIiKRBsz3IPbb9jehYRlYDDAwy5m2YDoWsS6WvQ7yLBfen+VZrn0deZbr6+2wvccS6S/cwdOQboN0Cjz4m07nzLdlXlnzqWDbVM72bQe+v9qjYMq5BW+uEsSOzfD4rYB+F6M0Okkk7dOx+N7E0unLwOLdrI+F9tfF+o5XvIt1oVesq3r1Rv26qWNe71++bQimCxXUChIkC7n/PI7nbaX+DxNt8rlKEEUx6hhY/G5m2j3zIjhLaD9bwHOm06Fyues6KxfeH3mWC+0/r3Lh/dG/29Gx/3TXr3Q36zv219b5utx9pNtytt3fl+fsL/TSCUn+YolMQowlMgkvFu9mPhEkx/B8HBLlEKsK7S+evT6WyCT8rPWd7KvbbXtYtwPdX5F640oQYVmXPeIlrYoMcO4RCSg3kXSSoLISTldJrC2/JNfTJNvbQVJKRglCpBQ6TkYUAKXvKuq/TjOba2brzGy9mS2KWH+kmf3WzJ43s8fMLBla9xkzezV4faaY9RQRkX0VLUGYWRxYApwBTALmm9mknGK3Are7+zTgZuB/B9seCtwEHAccC9xkZsOKVVcREdlXMXsQxwLr3f01d28B7gbOySkzCXgkmH40tP504GF33+ru7wAPA3OLWFcREclRzAQxCtgYmm8MloU9B7Q/m/VJoMbMhue5LWZ2uZk1mFlDU1NTwSouIiKlv0N2LfARM3sG+AiwCcj7QWN3X+ru9e5eX1sb+YNIIiKyn4r5FNMmYHRoPhks6+DubxL0IMysGvgHd3/XzDYBs3O2fayIdRURkRzF7EGsAcab2TgzGwRcADwQLmBmI8ysvQ43AMuC6QeB08xsWHBz+rRgmYiI9JKiJQh3TwFXkQnsa4F73f1FM7vZzM4Ois0G1pnZK8D7gP8VbLsV+CqZJLMGuDlYJiIivcTcB8ZX/uvr672hoaHU1RAR6VfM7Gl3r49aV+qb1CIi0kcpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhKpqAnCzOaa2TozW29miyLWjzGzR83sGTN73szODJYPMrOfmNkLZvacmc0uZj1FRGRfRUsQZhYHlgBnAJOA+WY2KafYjcC97l4HXAB8P1j+OQB3nwrMAb5pZurtiIj0omIG3WOB9e7+mru3AHcD5+SUcWBIMD0UeDOYngQ8AuDubwHvAvVFrKuIiOQoZoIYBWwMzTcGy8IWAxeZWSOwElgYLH8OONvMEmY2DjgGGJ17ADO73MwazKyhqamp0PUXETmolfqyzXxgubsngTOBnwaXkpaRSSgNwHeAJ4C23I3dfam717t7fW1tbe/VWkTkIJAo4r43kX3WnwyWhV0GzAVw99VmVgGMCC4rfam9kJk9AbxSxLqKiEiOYvYg1gDjzWycmQ0icxP6gZwybwCnApjZRKACaDKzKjMbHCyfA6Tc/aUi1lVERHIUrQfh7ikzuwp4EIgDy9z9RTO7GWhw9weAa4AfmtmXyNywXuDubmaHAQ+aWZpMr+PiYtVTRESimbuXug4FUV9f7w0NDaWuhohIv2JmT7t75FOipb5JLSIifZQShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGKmiDMbK6ZrTOz9Wa2KGL9GDN71MyeMbPnzezMYHmZma0wsxfMbK2Z3VDMeoqIyL6KliDMLA4sAc4AJgHzzWxSTrEbgXvdvQ64APh+sHweUO7uU4FjgM+b2dhi1VVERPZVzB7EscB6d3/N3VuAu4Fzcso4MCSYHgq8GVo+2MwSQCXQAmwvYl1FRCRHMRPEKGBjaL4xWBa2GLjIzBqBlcDCYPl9wHvAZuAN4FZ335p7ADO73MwazKyhqampwNUXETm4lfom9XxgubsngTOBn5pZjEzvow04AhgHXGNm78/d2N2Xunu9u9fX1tb2Zr1FRAa8RBH3vQkYHZpPBsvCLgPmArj7ajOrAEYAnwJ+4+6twFtm9kegHnitiPUVkT6stbWVxsZGdu/eXeqq9EsVFRUkk0nKysry3qaYCWINMN7MxpFJDBeQCfxhbwCnAsvNbCJQATQFy08h06MYDBwPfKeIdRWRPq6xsZGamhrGjh2LmZW6Ov2Ku7NlyxYaGxsZN25c3tsV7RKTu6eAq4AHgbVknlZ60cxuNrOzg2LXAJ8zs+eAnwEL3N3JPP1UbWYvkkk0P3H354tVVxHp+3bv3s3w4cOVHPaDmTF8+PAe976K2YPA3VeSufkcXvaV0PRLwKyI7ZrJPOoqItJByWH/7c97V+qb1CIi0kcpQYiISCQlCBGRPiaVSpW6CoAShIhIj3ziE5/gmGOOYfLkySxduhSA3/zmN8yYMYPp06dz6qmnAtDc3Mwll1zC1KlTmTZtGvfffz8A1dXVHfu67777WLBgAQALFizgiiuu4LjjjuO6667jqaee4oQTTqCuro4TTzyRdevWAdDW1sa1117LlClTmDZtGt/97nd55JFH+MQnPtGx34cffphPfvKTB9zWot6kFhEphn/95Yu89GZhR9+ZdMQQbvr45G7LLVu2jEMPPZRdu3Yxc+ZMzjnnHD73uc/x+OOPM27cOLZuzQz68NWvfpWhQ4fywgsvAPDOO+90u+/GxkaeeOIJ4vE427dv5/e//z2JRIJVq1bx5S9/mfvvv5+lS5eyYcMGnn32WRKJBFu3bmXYsGF84QtfoKmpidraWn7yk59w6aWXHtgbghKEiEiP3HbbbfziF78AYOPGjSxdupSTTjqp4/sFhx56KACrVq3i7rvv7thu2LBh3e573rx5xONxALZt28ZnPvMZXn31VcyM1tbWjv1eccUVJBKJrONdfPHF3HHHHVxyySWsXr2a22+//YDbqgQhIv1OPmf6xfDYY4+xatUqVq9eTVVVFbNnz+boo4/m5Zdfznsf4cdNc7+XMHjw4I7pf/mXf+Hkk0/mF7/4BRs2bGD27Nld7veSSy7h4x//OBUVFcybN68jgRwI3YMQEcnTtm3bGDZsGFVVVbz88ss8+eST7N69m8cff5y//vWvAB2XmObMmcOSJUs6tm2/xPS+972PtWvXkk6nO3oinR1r1KjM+KbLly/vWD5nzhx+8IMfdNzIbj/eEUccwRFHHMEtt9zCJZdcUpD2KkGIiORp7ty5pFIpJk6cyKJFizj++OOpra1l6dKlnHvuuUyfPp3zzz8fgBtvvJF33nmHKVOmMH36dB599FEAvva1r3HWWWdx4oknMnLkyE6Pdd1113HDDTdQV1eX9VTTZz/7WcaMGcO0adOYPn06d911V8e6Cy+8kNGjRzNx4sSCtNcyI1vkUdCsyt13FuSoRVBfX+8NDQ2lroaIFMnatWsLFvgGqquuuoq6ujouu+yyyPVR76GZPe3u9VHlu+1BmNmJZvYS8HIwP93Mvt/NZiIi0ouOOeYYnn/+eS666KKC7TOfuxjfBk4HHgBw9+fM7KSC1UBERA7Y008/XfB95nUPwt035ixqK3hNRESkT8mnB7HRzE4E3MzKgH8iM3y3iIgMYPn0IK4A/juZ35PeBBwdzIuIyADWbQ/C3d8GLuyFuoiISB+Sz1NMPzGzZbmv3qiciMhA19DQwBe/+MVO17/55pucd955vVijvfK5B/Gr0HQF8EngzeJUR0Skf2tra+sYTykf9fX11NdHfg0ByHxD+r777itE1Xqs2x6Eu98fet0J/CPQeWtERAaoDRs2cNRRR3HhhRcyceJEzjvvPHbu3MnYsWO5/vrrmTFjBj//+c956KGHOOGEE5gxYwbz5s2jubkZgDVr1nDiiScyffp0jj32WHbs2MFjjz3GWWedBcDvfvc7jj76aI4++mjq6urYsWMHGzZsYMqUKUBm7Kb2IcTr6uo6vp29fPlyzj33XObOncv48eO57rrrCtLe/RnNaTxwWEGOLiKyP369CP72QmH3efhUOONr3RZbt24dP/7xj5k1axaXXnop3/9+5nvDw4cP509/+hNvv/025557LqtWrWLw4MF8/etf51vf+haLFi3i/PPP55577mHmzJls376dysrKrH3feuutLFmyhFmzZtHc3ExFRUXW+iVLlmBmvPDCC7z88sucdtppvPLKKwA8++yzPPPMM5SXlzNhwgQWLlzI6NGjD+gtyecexA4z297+F/glcP0BHVVEpJ8aPXo0s2bNAuCiiy7iD3/4A0DHGExPPvkkL730ErNmzeLoo49mxYoVvP7666xbt46RI0cyc+ZMAIYMGbLPiKuzZs3i6quv5rbbbuPdd9/dZ/0f/vCHjm9KH3XUURx55JEdCeLUU09l6NChVFRUMGnSJF5//fUDbms+TzHVHPBRREQKKY8z/WIJD9cdnm8fqtvdmTNnDj/72c+yyrX/cFBXFi1axMc+9jFWrlzJrFmzePDBB/fpRXSmvLy8YzoejxfkZ0s77UGY2YyuXgd8ZBGRfuiNN95g9erVANx111186EMfylp//PHH88c//pH169cD8N577/HKK68wYcIENm/ezJo1awDYsWPHPkH8L3/5C1OnTuX6669n5syZ+/zOxIc//GHuvPNOAF555RXeeOMNJkyYUJR2Qtc9iG92sc6BUwpcFxGRPm/ChAksWbKESy+9lEmTJnHllVfy3e9+t2N9bW0ty5cvZ/78+ezZsweAW265hQ9+8IPcc889LFy4kF27dlFZWcmqVauy9v2d73yHRx99lFgsxuTJkznjjDPYvHlzx/ovfOELXHnllUydOpVEIsHy5cuzeg6Flvdw3/u1c7O5wL8BceBH7v61nPVjgBXAIUGZRe6+0swuBP5nqOg0YIa7P9vZsTTct8jA1heG+96wYQNnnXUWf/7zn0taj/3V0+G+83qKycymAJPIfA8CAHfv8gdPzSwOLAHmAI3AGjN7wN1fChW7EbjX3f/dzCYBK4GxweO0dwb7mQr8Z1fJQURECq/bBGFmNwGzySSIlcAZwB+A7n4R+1hgvbu/FuznbuAcIJwgHBgSTA8l+gt484G7I5aLiPSqsWPH9tvew/7IZ7C+84BTgb+5+yXAdDLBvDujgPAw4Y3BsrDFwEVm1kgm+SyM2M/5wM8ilmNml5tZg5k1NDU15VElERHJVz4JYre7p4GUmQ0B3gIO7NsXe80Hlrt7EjgT+KmZddTJzI4Ddrp7ZMp296XuXu/u9bW1tQWqkoiIQBeXmMxsCZkz96fM7BDgh8DTQDOwOo99byI7kSSDZWGXAXMB3H21mVUAI8gkIYAL6KT3ICIixdXVPYhXgP8DHAG8RyZQzwGGuPvzeex7DTDezMaRSQwXAJ/KKfMGmctXy81sIpmb4E0AQU/iH4EP590aEREpmE4vMbn7v7n7CcBJwBZgGfAb4JNmNr67Hbt7CrgKeJDML9Dd6+4vmtnNZnZ2UOwa4HNm9hyZBLTA9z53exKwsf0mt4jIQLR8+XKuuuoqABYvXsytt95a4hrtlc9QG68DXwe+bmZ1ZBLFV8h8b6G7bVeSufkcXvaV0PRLwKxOtn0MOL67Y4iIlIK74+7EYvncyu2f8hmsL2FmHzezO4FfA+uAc4teMxGRPmbDhg1MmDCBT3/600yZMoWvfvWrzJw5k2nTpnHTTTd1lLv99tuZNm0a06dP5+KLLwbgl7/8Jccddxx1dXV89KMf5e9//3upmpG3rm5SzyHzlNGZwFNkvotwubu/10t1ExGJ9PWnvs7LW1/uvmAPHHXoUVx/bPcDVb/66qusWLGC7du3c9999/HUU0/h7px99tk8/vjjDB8+nFtuuYUnnniCESNGsHXrVgA+9KEP8eSTT2Jm/OhHP+Ib3/gG3/xmVyMalV5Xl5huAO4CrnH3d3qpPiIifdqRRx7J8ccfz7XXXstDDz1EXV0dAM3Nzbz66qs899xzzJs3jxEjRgBw6KGHAtDY2Mj555/P5s2baWlpYdy4cSVrQ746TRDursH4RKRPyudMv1jCw3rfcMMNfP7zn89aHx64L2zhwoVcffXVnH322Tz22GMsXry42FU9YAP37oqISBGdfvrpLFu2rOPnRDdt2sRbb73FKaecws9//nO2bNkC0HGJadu2bYwalRlMYsWKFaWpdA/tz0+Oiogc9E477TTWrl3LCSecAEB1dTV33HEHkydP5p//+Z/5yEc+Qjwep66ujuXLl7N48WLmzZvHsGHDOOWUU/jrX/9a4hZ0r6jDffcmDfctMrD1heG++7ueDvetS0wiIhJJCUJERCIpQYhIvzFQLomXwv68d0oQItIvVFRUsGXLFiWJ/eDubNmyhYqKiu4Lh+gpJhHpF5LJJI2NjejHwfZPRUUFyWSyR9soQYhIv1BWVtYvvn08kOgSk4iIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpH0PQgRkT4onXZa2tK0tqVpbXNaUpnpPcHf9vmWtjSHDh7EUYcPKXgdlCBE5KDk7pnA25amNZUJtC3B39a2NK0pp6WtjZZUdpnWcLmOZd4RuMPlMsucllQbrW0eHeDby+QsS6XzH1LkrGkj+d6nZhT8PVKCEJGicHdSac8Kei3dnA1nB9Xc7dI5yzxru5ZQkG8/s25N5QTlnKBeaGVxY1A8RlkilvkbjzGofTphHcuqyxMMqtq7fm8565gPbxsu036M8LLamvKCtwWUIET6FHenLZ0JrKm0kwoCWVs6E+hSaactve+yVJuTSoem25en08G60LI2pzWdpq3NaQ32l2rzYJ/pzLKgTCp0nLZ0pnz4bHifoJxzNlxoiVh2AC1vD5iJ7GBbNSjBIR3r4pTFLSibXa48FHDLspbFsgL9oIRlB/OswL93H2ZW8DaXUlEThJnNBf4NiAM/cvev5awfA6wADgnKLHL3lcG6acAPgCFAGpjp7ruLWV/pH9LpIMClvSNQptqyA1tH8MwNjjmBcm8ADQXPIBiGy3QEz2A+fOys40Qdu5MgnRXQQ2V6WyJmxGOZAJj5ayRie6fb1yXiRjwWozweo6IsxpCKRPYZcoHOhsNBOiv4x2PEYgMrAPd1RUsQZhYHlgBzgEZgjZk94O4vhYrdCNzr7v9uZpOAlcBYM0sAdwAXu/tzZjYcaC1WXaW4WlJptu9uZduuVrbvCv7uTnXMb9/VGlofLN/dyq6Wtsig2tsx1AzKgoCZiIcCacxIxGMkguXxWCwIrpkAOygRo7I9uAZlErFY8De0bbBdPGd/ZVlBOhYcOzjOPscOHSfqGPHs47S3IRGzAXfWK4VTzB7EscB6d38NwMzuBs4BwgnCyfQQAIYCbwbTpwHPu/tzAO6+pYj1lG64O817UpmgvrM1Mth3THcs2xvwd7W2dbn/QYkYQyvLGFKRYGhlGcOrB/H+2sFUlsX3BtXc4BkKcOEz371BOuIMOBQow2fF7fsui0UHaZ21ysGqmAliFLAxNN8IHJdTZjHwkJktBAYDHw2WfxBwM3sQqAXudvdv5B7AzC4HLgcYM2ZMQSs/0OzvWXz7+q7O2s2gpjzBkMqyINCX8f4R1QypTHTMD60K/laWZS0fUllGRVm8994IEclbqW9SzweWu/s3zewE4KdmNiWo14eAmcBO4Ldm9rS7/za8sbsvBZYC1NfXD+ifmWo/i+8sgG/PCfg9PYsvT8RCAT7BiOAsviPARwT2zLIyqssTxHWWLTLgFDNBbAJGh+aTwbKwy4C5AO6+2swqgBFkehuPu/vbAGa2EpgB/JZ+rCWV3iewhy/RRAX29vl8z+LDZ+rvH1GdHdhDZ/iZ6cxZ/5AKncWLyL6KmSDWAOPNbByZxHAB8KmcMm8ApwLLzWwiUAE0AQ8C15lZFdACfAT4dhHrmpd02mluCV9vzw7gUZduwgF/d2vXj/2Vt1+LDwL5iOpB/LfawVmBvT3gD8maL6OmPKFr5SJSUEVLEO6eMrOryAT7OLDM3V80s5uBBnd/ALgG+KGZfYnMDesFnvlF8nfM7FtkkowDK939/xWjnlvfa+H3rzZ1e4lm265Wduzu/iw+c3a+91LMBw6rDl2D33smPyQ34OssXkT6GMvE4/6vvr7eGxoaerzdcxvf5Zwlf+yYzzzfXZZzSSb6Ek3uTdjqQTqLF5H+Jbi/Wx+1rtQ3qUtuwuE1rLr6IwytLKOmIqGzeBGRwEGfICrK4nzgsOpSV0NEpM/R70GIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEVNEGY218zWmdl6M1sUsX6MmT1qZs+Y2fNmdmawfKyZ7TKzZ4PXfxSzniIisq9EsXZsZnFgCTAHaATWmNkD7v5SqNiNwL3u/u9mNglYCYwN1v3F3Y8uVv1ERKRrxexBHAusd/fX3L0FuBs4J6eMA0OC6aHAm0Wsj4iI9EAxE8QoYGNovjFYFrYYuMjMGsn0HhaG1o0LLj39zsw+HHUAM7vczBrMrKGpqamAVRcRkVLfpJ4PLHf3JHAm8FMziwGbgTHuXgdcDdxlZkNyN3b3pe5e7+71tbW1vVpxEZGBrpgJYhMwOjSfDJaFXQbcC+Duq4EKYIS773H3LcHyp4G/AB8sYl1FRCRH0W5SA2uA8WY2jkxiuAD4VE6ZN4BTgeVmNpFMgmgys1pgq7u3mdn7gfHAa0Wsq4hIn5H2NHva9rAntSfzN/TandpNS1sLu9v2/h05eCTHjTyu4PUoWoJw95SZXQU8CMSBZe7+opndDDS4+wPANcAPzexLZG5YL3B3N7OTgJvNrBVIA1e4+9Zi1VVEJIq705Ju6QjW4aDc0tbC7tTufQJ4T8p1Fvhb0609qufpY08vSoIwdy/4Tkuhvr7eGxoaSl0NESkCdyflqf0O0lFn4vmcne9p24Oz/zFyUGwQ5fFyyhPlmb/hV6KcingFg+KDqIhXRJapSOxd31W5mkE1DC0ful91NLOn3b0+al0xLzGJyADUlm7bJ6jmG3y7Ktdd4E97er/rnLDEPoE1HHyryqq6DtKxQVnlOwvyg+KZcuXxcgbFBxGzUj8HdGCUIEQGoPZLI7tad7EztZNdqV3sbN2ZNb0rlf+68HwqndrvesUs1uUZ8pBBQ6iN13Z61t0efHPPxLsqNyg+iERMoW5/6F0TKSF3Z0/bnu4DdWeBPli3K7Vrn3Vt3pZ3PRKWoLKskqpEFVVlVVQmMtPDK4YzumZ0x3xlorLLs+buLqEkLIGZFfEdlUJSghDJg7vvDcJdBPPOAnpn2+1K7erRpZOyWBlVZVUdwboqUUVlWSWHVR2WmQ+vCwX6febLKrP2URYvK+K7J/2VEoQMKGlPszu1OytIdxWwu7uc0h7Md6d29+hmZXm8PDIwD60amh2cuwjiuQG9MlFJWUyBXHqPEoT0KdtbtrNxx0YadzTyzu53enSNvP3VE+03KCsTlVkBeljFsLzOyDtbF4/Fi/QOifQeJQjpVal0ir+99zcamxtp3NHYkQza57e3bI/cLuose3BiMCMqRnQeqNuvqedeUgnKVMQrFMhFuqAEIQW3o2XHPoG/PRlsfm9z1s3TRCzBqOpRJKuTTB0xldE1o0lWJ0nWJBleOZyqRBUViYp+/7igSH+kBCE9lkqn+PvOv2cF/o5E0NzItj3bssoPKx9GsiaTAM4YdwbJmmRHIjis6jCdxYv0UUoQEqm5pTk78AfBf+OOjWxu3kzK9z4Ln7AER1QfQbImyZQRUzp6AMmaJKOqR1EzqKaELRGR/aUEcZBqS7ft7QU0h+4FBPPv7nk3q/wh5YeQrE4yefhkTh97OsnqoBdQk+R9Ve9TL0BkAFKCGMCaW5rZ1Lxpn/sBG3ds5M333sz6RmzCEoysHkmyOslpR57W0QNo7w2oFyBy8FGC6Mfa0m28tfOtTp8IemfPO1nlh5YPJVmdZOLwicw5cs7eewFBL0DDEYhImCJCH/de63v73ANobG5k045NbGrelDUscNzijBw8kmRNko8e+dGsHkCyJsmQQfv8KJ+ISKeUIEos7Wne2vlW5GOhjc2NbN2d/TMYQwYNIVmT5IPDPsgpY07JeiLo8MGHqxcgIgWjaNILdrbu7PQyUFQv4PDBh5OsSWYSQKgHkKxO7veY7yIiPaUEUQDtvYCsHkDoyaDcXkBNWQ3JmiTjh43n5DEnZz0RdPjgwzXejoj0CUoQedrZupNNzZsivxi2accmWtItHWVjFsvcC6hOcvLokzt6AKOrM0lAvQAR6Q+UIAJpT9O0sykr8IcvCW3ZvSWrfHVZNaNrRvOBQz7A7OTsrCRweLV6ASLS/x30CWLtlrUs+v0iNjVvYk/bno7lMYtxeFXmXsDs0bOznwgK7gXoh09EZCA76BPEIeWH8P6h7+ek5ElZ9wJGDh6pH1ERkYPaQZ8gRlaP5Nsnf7vU1RAR6XM0hrKIiERSghARkUhFTRBmNtfM1pnZejNbFLF+jJk9ambPmNnzZnZmxPpmM7u2mPUUEZF9FS1BmFkcWAKcAUwC5pvZpJxiNwL3unsdcAHw/Zz13wJ+Xaw6iohI54rZgzgWWO/ur7l7C3A3cE5OGQfaR5AbCrzZvsLMPgH8FXixiHUUEZFOFDNBjAI2huYbg2Vhi4GLzKwRWAksBDCzauB64F+7OoCZXW5mDWbW0NTUVKh6i4gIpb9JPR9Y7u5J4Ezgp2YWI5M4vu3uzV1t7O5L3b3e3etra2uLX1sRkYNIMb8HsQkYHZpPBsvCLgPmArj7ajOrAEYAxwHnmdk3gEOAtJntdvfvFbG+IiISYu5enB2bJYBXgFPJJIY1wKfc/cVQmV8D97j7cjObCPwWGOWhSpnZYqDZ3W/t5nhNwOsHUOURwNsHsH1fMVDaAWpLXzRQ2gFqS7sj3T3yEkzRehDunjKzq4AHgTiwzN1fNLObgQZ3fwC4BvihmX2JzA3rBb6fGauzBubLzBrcvf5A9tEXDJR2gNrSFw2UdoDako+iDrXh7ivJ3HwOL/tKaPolYFY3+1hclMqJiEiXSn2TWkRE+igliL2WlroCBTJQ2gFqS180UNoBaku3inaTWkRE+jf1IEREJJIShIiIRDqoEoSZLTOzt8zsz52sNzO7LRh99nkzm9HbdcxHHu2YbWbbzOzZ4PWVqHJ9gZmNDkb0fcnMXjSzf4oo0+c/lzzb0S8+FzOrMLOnzOy5oC37DHljZuVmdk/wmfyXmY0tQVW7lWdbFphZU+hz+Wwp6poPM4sHo1//KmJd4T8Tdz9oXsBJwAzgz52sP5PM6LEGHA/8V6nrvJ/tmA38qtT1zLMtI4EZwXQNmS9XTupvn0ue7egXn0vwPlcH02XAfwHH55T5AvAfwfQFZL7wWvK672dbFgDfK3Vd82zP1cBdUf+OivGZHFQ9CHd/HNjaRZFzgNs940ngEDMb2Tu1y18e7eg33H2zu/8pmN4BrGXfQR37/OeSZzv6heB9bh8HrSx45T7Ncg6wIpi+DzjVzKyXqpi3PNvSL5hZEvgY8KNOihT8MzmoEkQe8hmBtr84IehW/9rMJpe6MvkIusR1ZM7ywvrV59JFO6CffC7BpYxngbeAh92908/E3VPANmB4r1YyT3m0BeAfgsuX95nZ6Ij1fcF3gOuAdCfrC/6ZKEEMTH8iM77KdOC7wH+WtjrdC4Z4vx/4H+6+vdT12V/dtKPffC7u3ubuR5MZZPNYM5tS4irttzza8ktgrLtPAx5m71l4n2FmZwFvufvTvXlcJYhs+YxA2+e5+/b2brVnhjspM7MRJa5Wp8ysjExQvdPd/29EkX7xuXTXjv72uQC4+7vAowSjLod0fCbBwJxDgS29Wrke6qwt7r7F3fcEsz8CjunlquVjFnC2mW0g8+Nrp5jZHTllCv6ZKEFkewD4dPDUzPHANnffXOpK9ZSZHd5+7dHMjiXzOffJ/7xBPX8MrHX3b3VSrM9/Lvm0o798LmZWa2aHBNOVwBzg5ZxiDwCfCabPAx7x4O5oX5JPW3LuZ51N5v5Rn+LuN7h70t3HkrkB/Yi7X5RTrOCfSVEH6+trzOxnZJ4kGWGZX7G7icxNK9z9P8gMLHgmsB7YCVxSmpp2LY92nAdcaWYpYBdwQV/8zxuYBVwMvBBcJwb4MjAG+tXnkk87+svnMhJYYZnflY+R+d34X1n2SMw/JvMDX+vJPDBxQemq26V82vJFMzsbSJFpy4KS1baHiv2ZaKgNERGJpEtMIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIER6wMzaQqN+Pmtmiwq477HWyQi9IqVwUH0PQqQAdgXDNogMeOpBiBSAmW0ws2+Y2QvB7w98IFg+1sweCQaC+62ZjQmWv8/MfhEM3PecmZ0Y7CpuZj8MfrvgoeDbvyIloQQh0jOVOZeYzg+t2+buU4HvkRl5EzKD8q0IBoK7E7gtWH4b8Ltg4L4ZwIvB8vHAEnefDLwL/ENRWyPSBX2TWqQHzKzZ3asjlm8ATnH314JB+/7m7sPN7G1gpLu3Bss3u/sIM2sCkqFB4tqHCX/Y3ccH89cDZe5+Sy80TWQf6kGIFI53Mt0Te0LTbeg+oZSQEoRI4Zwf+rs6mH6CvYOmXQj8Ppj+LXAldPygzdDeqqRIvnR2ItIzlaHRWgF+4+7tj7oOM7PnyfQC5gfLFgI/MbP/CTSxdyTafwKWmtllZHoKVwJ9aghzEd2DECmA4B5Evbu/Xeq6iBSKLjGJiEgk9SBERCSSehAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikf4/xZGRE/svp/AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.003\n",
    "epochs = 5\n",
    "batch_size=32\n",
    "classification_threshold = 0.8\n",
    "label_name = 'label'\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', \n",
    "        threshold=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Precision(\n",
    "        name='precision',\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Recall(\n",
    "        name=\"recall\",\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Rename the column name to match the expect input. \n",
    "train.rename(columns={\"file_type\":'file_type_input'}, inplace=True)\n",
    "valid.rename(columns={\"file_type\":'file_type_input'}, inplace=True)\n",
    "test.rename(columns={\"file_type\":'file_type_input'}, inplace=True)\n",
    "\n",
    "model = Build_Model(learning_rate, hub_layer, metrics)\n",
    "\n",
    "epochs, hist = Train_Model(model, train[['file_type_input','label']], epochs, label_name, valid[['file_type_input', 'label']], batch_size)\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "Plot_Curve(epochs, hist, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in train[['file_type_input','label']].items()}\n",
    "label = np.array(features.pop(label_name))\n",
    "type_pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=float32, numpy=\n",
       "array([[ 0.5452786 , -0.3172977 , -0.07704936,  0.2036959 ,  0.38697574,\n",
       "        -0.1419868 , -0.13361807,  0.12941073,  0.1000542 , -0.24626689,\n",
       "        -0.17147727,  0.12623933, -0.16933653,  0.16426836,  0.19530015,\n",
       "         0.33890274,  0.28310332,  0.30670217, -0.43901318, -0.01623722,\n",
       "        -0.04128662,  0.04714517, -0.03072401, -0.23457187, -0.03094403,\n",
       "        -0.46686676,  0.04451368,  0.19684725, -0.24635707, -0.05368439,\n",
       "        -0.03086434,  0.10913029, -0.20867568, -0.06583074,  0.34751582,\n",
       "        -0.19124426, -0.14331762,  0.05113909, -0.18654865, -0.03709022,\n",
       "        -0.00354105,  0.03693081, -0.21767217, -0.1720672 ,  0.0308617 ,\n",
       "         0.16607574, -0.18863867,  0.10008863, -0.06710914,  0.27585688]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'imports'\n",
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(\n",
    "    embedding, \n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=True,\n",
    "    name=layer_name\n",
    ")\n",
    "hub_layer(train['imports'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858/1858 [==============================] - 53s 28ms/step - loss: 0.2874 - accuracy: 0.8757 - precision: 0.9694 - recall: 0.8066 - val_loss: 0.2242 - val_accuracy: 0.8922 - val_precision: 0.9779 - val_recall: 0.8286\n",
      "Epoch 2/5\n",
      "1858/1858 [==============================] - 53s 28ms/step - loss: 0.2143 - accuracy: 0.9041 - precision: 0.9785 - recall: 0.8499 - val_loss: 0.2109 - val_accuracy: 0.9120 - val_precision: 0.9749 - val_recall: 0.8671\n",
      "Epoch 3/5\n",
      "1858/1858 [==============================] - 53s 29ms/step - loss: 0.2008 - accuracy: 0.9087 - precision: 0.9810 - recall: 0.8559 - val_loss: 0.2056 - val_accuracy: 0.8970 - val_precision: 0.9841 - val_recall: 0.8317\n",
      "Epoch 4/5\n",
      "1858/1858 [==============================] - 53s 28ms/step - loss: 0.1887 - accuracy: 0.9124 - precision: 0.9829 - recall: 0.8609 - val_loss: 0.2102 - val_accuracy: 0.9128 - val_precision: 0.9695 - val_recall: 0.8737\n",
      "Epoch 5/5\n",
      "1858/1858 [==============================] - 53s 29ms/step - loss: 0.1825 - accuracy: 0.9146 - precision: 0.9834 - recall: 0.8643 - val_loss: 0.2035 - val_accuracy: 0.9119 - val_precision: 0.9789 - val_recall: 0.8633\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.003\n",
    "epochs = 5\n",
    "batch_size=32\n",
    "classification_threshold = 0.8\n",
    "label_name = 'label'\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', \n",
    "        threshold=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Precision(\n",
    "        name='precision',\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "    tf.keras.metrics.Recall(\n",
    "        name=\"recall\",\n",
    "        thresholds=classification_threshold\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Rename the column name to match the expect input. \n",
    "train.rename(columns={\"imports\":'imports_input'}, inplace=True)\n",
    "valid.rename(columns={\"imports\":'imports_input'}, inplace=True)\n",
    "test.rename(columns={\"imports\":'imports_input'}, inplace=True)\n",
    "\n",
    "model = Build_Model(learning_rate, hub_layer, metrics)\n",
    "\n",
    "epochs, hist = Train_Model(model, train[['imports_input','label']], epochs, label_name, valid[['imports_input', 'label']], batch_size)\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "Plot_Curve(epochs, hist, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in train[['imports_input','label']].items()}\n",
    "label = np.array(features.pop(label_name))\n",
    "imports_pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>79603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rdata</td>\n",
       "      <td>47485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>68845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sxdata</td>\n",
       "      <td>2685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rsrc</td>\n",
       "      <td>84529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reloc</td>\n",
       "      <td>55883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ndata</td>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rdatap</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brdata</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tc</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>upx0</td>\n",
       "      <td>6410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>upx1</td>\n",
       "      <td>6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pdata</td>\n",
       "      <td>12526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tls</td>\n",
       "      <td>7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>idata</td>\n",
       "      <td>12489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>crt</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sedata</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>upx2</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gfids</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  count\n",
       "0     text  79603\n",
       "1    rdata  47485\n",
       "2     data  68845\n",
       "3   sxdata   2685\n",
       "4     rsrc  84529\n",
       "5    reloc  55883\n",
       "6    ndata   1601\n",
       "7   rdatap    524\n",
       "8   brdata    526\n",
       "9       tc    471\n",
       "10    upx0   6410\n",
       "11    upx1   6316\n",
       "12   pdata  12526\n",
       "13  data32      3\n",
       "14     tls   7472\n",
       "15   idata  12489\n",
       "16     crt   1607\n",
       "17  sedata     17\n",
       "18    upx2    665\n",
       "19   gfids    941"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "sections_column = [ast.literal_eval(i) for i in df['sections']]\n",
    "section_dict ={} #Dict of of all unique section names with key=section name and value=count of section in the data\n",
    "for entry in sections_column:\n",
    "    for section in entry:\n",
    "        try:\n",
    "            section_name = ''.join(e for e in section[0] if e.isalnum()).lower()\n",
    "        except:\n",
    "            section_name = ''.join(e for e in section[0].decode('utf8', 'ignore') if e.isalnum()).lower()\n",
    "        try:\n",
    "            duplicate = 0\n",
    "            section_dict[section_name] += 1\n",
    "        except KeyError:\n",
    "            section_dict[section_name] = 1\n",
    "summmary_df = pd.DataFrame(data=[section_dict.keys(),section_dict.values()]).T\n",
    "summmary_df.columns = ['name','count']\n",
    "summmary_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100.00%\r"
     ]
    }
   ],
   "source": [
    "TOP_PERCENTILE = 95 #About 40 sections comprise 95% of the data, can increase the granularity but this explodes the feature space and takes WAY too long to finish\n",
    "summmary_df = summmary_df.sort_values(by='count',ascending=False).reset_index(drop=True)\n",
    "summmary_df['%_of_total'] = summmary_df['count']/sum(summmary_df['count'])*100\n",
    "summmary_df['cumulative_%'] = summmary_df['%_of_total'].cumsum()\n",
    "\n",
    "top_sections = summmary_df[summmary_df['cumulative_%']<TOP_PERCENTILE]['name'].values.tolist()\n",
    "col_names = ['r_size','v_size','code','executable','writable']\n",
    "final_df = pd.DataFrame()\n",
    "df_list = []\n",
    "\n",
    "for row,section_sample in enumerate(sections_column):\n",
    "    merged_df = pd.DataFrame()\n",
    "    completed_sections = []\n",
    "    for i in section_sample:\n",
    "        try:\n",
    "            i[0] = ''.join(e for e in i[0] if e.isalnum()).lower()\n",
    "        except:\n",
    "            i[0] = ''.join(e for e in i[0].decode('utf8', 'ignore') if e.isalnum()).lower()\n",
    "        if (i[0] in completed_sections) or (i[0] not in top_sections):\n",
    "            continue\n",
    "        completed_sections.append(i[0])\n",
    "        temp_df = pd.DataFrame([i[1:]],columns=col_names).add_prefix(f'{i[0]}_')\n",
    "        merged_df = pd.concat([merged_df,temp_df],axis=1)\n",
    "    df_list.append(merged_df)\n",
    "#     final_df = pd.concat([final_df,merged_df],ignore_index=True).fillna(0)\n",
    "    print(f'Completed {row/len(sections_column)*100:0.2f}%',end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_r_size</th>\n",
       "      <th>text_v_size</th>\n",
       "      <th>text_code</th>\n",
       "      <th>text_executable</th>\n",
       "      <th>text_writable</th>\n",
       "      <th>rdata_r_size</th>\n",
       "      <th>rdata_v_size</th>\n",
       "      <th>rdata_code</th>\n",
       "      <th>rdata_executable</th>\n",
       "      <th>rdata_writable</th>\n",
       "      <th>data_r_size</th>\n",
       "      <th>data_v_size</th>\n",
       "      <th>data_code</th>\n",
       "      <th>data_executable</th>\n",
       "      <th>data_writable</th>\n",
       "      <th>sxdata_r_size</th>\n",
       "      <th>sxdata_v_size</th>\n",
       "      <th>sxdata_code</th>\n",
       "      <th>sxdata_executable</th>\n",
       "      <th>sxdata_writable</th>\n",
       "      <th>rsrc_r_size</th>\n",
       "      <th>rsrc_v_size</th>\n",
       "      <th>rsrc_code</th>\n",
       "      <th>rsrc_executable</th>\n",
       "      <th>rsrc_writable</th>\n",
       "      <th>reloc_r_size</th>\n",
       "      <th>reloc_v_size</th>\n",
       "      <th>reloc_code</th>\n",
       "      <th>reloc_executable</th>\n",
       "      <th>reloc_writable</th>\n",
       "      <th>ndata_r_size</th>\n",
       "      <th>ndata_v_size</th>\n",
       "      <th>ndata_code</th>\n",
       "      <th>ndata_executable</th>\n",
       "      <th>ndata_writable</th>\n",
       "      <th>rdatap_r_size</th>\n",
       "      <th>rdatap_v_size</th>\n",
       "      <th>rdatap_code</th>\n",
       "      <th>rdatap_executable</th>\n",
       "      <th>rdatap_writable</th>\n",
       "      <th>brdata_r_size</th>\n",
       "      <th>brdata_v_size</th>\n",
       "      <th>brdata_code</th>\n",
       "      <th>brdata_executable</th>\n",
       "      <th>brdata_writable</th>\n",
       "      <th>upx0_r_size</th>\n",
       "      <th>upx0_v_size</th>\n",
       "      <th>upx0_code</th>\n",
       "      <th>upx0_executable</th>\n",
       "      <th>upx0_writable</th>\n",
       "      <th>upx1_r_size</th>\n",
       "      <th>upx1_v_size</th>\n",
       "      <th>upx1_code</th>\n",
       "      <th>upx1_executable</th>\n",
       "      <th>upx1_writable</th>\n",
       "      <th>pdata_r_size</th>\n",
       "      <th>pdata_v_size</th>\n",
       "      <th>pdata_code</th>\n",
       "      <th>pdata_executable</th>\n",
       "      <th>pdata_writable</th>\n",
       "      <th>tls_r_size</th>\n",
       "      <th>tls_v_size</th>\n",
       "      <th>tls_code</th>\n",
       "      <th>tls_executable</th>\n",
       "      <th>tls_writable</th>\n",
       "      <th>idata_r_size</th>\n",
       "      <th>idata_v_size</th>\n",
       "      <th>idata_code</th>\n",
       "      <th>idata_executable</th>\n",
       "      <th>idata_writable</th>\n",
       "      <th>crt_r_size</th>\n",
       "      <th>crt_v_size</th>\n",
       "      <th>crt_code</th>\n",
       "      <th>crt_executable</th>\n",
       "      <th>crt_writable</th>\n",
       "      <th>upx2_r_size</th>\n",
       "      <th>upx2_v_size</th>\n",
       "      <th>upx2_code</th>\n",
       "      <th>upx2_executable</th>\n",
       "      <th>upx2_writable</th>\n",
       "      <th>gfids_r_size</th>\n",
       "      <th>gfids_v_size</th>\n",
       "      <th>gfids_code</th>\n",
       "      <th>gfids_executable</th>\n",
       "      <th>gfids_writable</th>\n",
       "      <th>code_r_size</th>\n",
       "      <th>code_v_size</th>\n",
       "      <th>code_code</th>\n",
       "      <th>code_executable</th>\n",
       "      <th>code_writable</th>\n",
       "      <th>bss_r_size</th>\n",
       "      <th>bss_v_size</th>\n",
       "      <th>bss_code</th>\n",
       "      <th>bss_executable</th>\n",
       "      <th>bss_writable</th>\n",
       "      <th>didat_r_size</th>\n",
       "      <th>didat_v_size</th>\n",
       "      <th>didat_code</th>\n",
       "      <th>didat_executable</th>\n",
       "      <th>didat_writable</th>\n",
       "      <th>aspack_r_size</th>\n",
       "      <th>aspack_v_size</th>\n",
       "      <th>aspack_code</th>\n",
       "      <th>aspack_executable</th>\n",
       "      <th>aspack_writable</th>\n",
       "      <th>xdata_r_size</th>\n",
       "      <th>xdata_v_size</th>\n",
       "      <th>xdata_code</th>\n",
       "      <th>xdata_executable</th>\n",
       "      <th>xdata_writable</th>\n",
       "      <th>edata_r_size</th>\n",
       "      <th>edata_v_size</th>\n",
       "      <th>edata_code</th>\n",
       "      <th>edata_executable</th>\n",
       "      <th>edata_writable</th>\n",
       "      <th>itext_r_size</th>\n",
       "      <th>itext_v_size</th>\n",
       "      <th>itext_code</th>\n",
       "      <th>itext_executable</th>\n",
       "      <th>itext_writable</th>\n",
       "      <th>adata_r_size</th>\n",
       "      <th>adata_v_size</th>\n",
       "      <th>adata_code</th>\n",
       "      <th>adata_executable</th>\n",
       "      <th>adata_writable</th>\n",
       "      <th>didata_r_size</th>\n",
       "      <th>didata_v_size</th>\n",
       "      <th>didata_code</th>\n",
       "      <th>didata_executable</th>\n",
       "      <th>didata_writable</th>\n",
       "      <th>rmnet_r_size</th>\n",
       "      <th>rmnet_v_size</th>\n",
       "      <th>rmnet_code</th>\n",
       "      <th>rmnet_executable</th>\n",
       "      <th>rmnet_writable</th>\n",
       "      <th>tsustub_r_size</th>\n",
       "      <th>tsustub_v_size</th>\n",
       "      <th>tsustub_code</th>\n",
       "      <th>tsustub_executable</th>\n",
       "      <th>tsustub_writable</th>\n",
       "      <th>tsuarch_r_size</th>\n",
       "      <th>tsuarch_v_size</th>\n",
       "      <th>tsuarch_code</th>\n",
       "      <th>tsuarch_executable</th>\n",
       "      <th>tsuarch_writable</th>\n",
       "      <th>orpc_r_size</th>\n",
       "      <th>orpc_v_size</th>\n",
       "      <th>orpc_code</th>\n",
       "      <th>orpc_executable</th>\n",
       "      <th>orpc_writable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29696.0</td>\n",
       "      <td>29442.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>38006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>36828.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13312.0</td>\n",
       "      <td>12936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196608.0</td>\n",
       "      <td>194161.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262144.0</td>\n",
       "      <td>261943.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89600.0</td>\n",
       "      <td>89294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7680.0</td>\n",
       "      <td>11404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>749568.0</td>\n",
       "      <td>749176.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15872.0</td>\n",
       "      <td>15488.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23552.0</td>\n",
       "      <td>23364.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_r_size  text_v_size  text_code  text_executable  text_writable  \\\n",
       "0      29696.0      29442.0        1.0              1.0            0.0   \n",
       "1     196608.0     194161.0        1.0              1.0            0.0   \n",
       "2     262144.0     261943.0        1.0              1.0            0.0   \n",
       "3      23552.0      23364.0        1.0              1.0            0.0   \n",
       "4       2048.0       1848.0        1.0              1.0            0.0   \n",
       "\n",
       "   rdata_r_size  rdata_v_size  rdata_code  rdata_executable  rdata_writable  \\\n",
       "0       38400.0       38006.0         0.0               0.0             0.0   \n",
       "1           0.0           0.0         0.0               0.0             0.0   \n",
       "2       89600.0       89294.0         0.0               0.0             0.0   \n",
       "3           0.0           0.0         0.0               0.0             0.0   \n",
       "4           0.0           0.0         0.0               0.0             0.0   \n",
       "\n",
       "   data_r_size  data_v_size  data_code  data_executable  data_writable  \\\n",
       "0       1024.0      36828.0        0.0              0.0            1.0   \n",
       "1          0.0          0.0        0.0              0.0            0.0   \n",
       "2       7680.0      11404.0        0.0              0.0            1.0   \n",
       "3          0.0          0.0        0.0              0.0            0.0   \n",
       "4          0.0          0.0        0.0              0.0            0.0   \n",
       "\n",
       "   sxdata_r_size  sxdata_v_size  sxdata_code  sxdata_executable  \\\n",
       "0          512.0            8.0          0.0                0.0   \n",
       "1            0.0            0.0          0.0                0.0   \n",
       "2            0.0            0.0          0.0                0.0   \n",
       "3            0.0            0.0          0.0                0.0   \n",
       "4            0.0            0.0          0.0                0.0   \n",
       "\n",
       "   sxdata_writable  rsrc_r_size  rsrc_v_size  rsrc_code  rsrc_executable  \\\n",
       "0              1.0      13312.0      12936.0        0.0              0.0   \n",
       "1              0.0       4096.0       1960.0        0.0              0.0   \n",
       "2              0.0     749568.0     749176.0        0.0              0.0   \n",
       "3              0.0       1536.0       1304.0        0.0              0.0   \n",
       "4              0.0       1536.0       1232.0        0.0              0.0   \n",
       "\n",
       "   rsrc_writable  reloc_r_size  reloc_v_size  reloc_code  reloc_executable  \\\n",
       "0            0.0        1536.0        1274.0         0.0               0.0   \n",
       "1            0.0           0.0           0.0         0.0               0.0   \n",
       "2            0.0       15872.0       15488.0         0.0               0.0   \n",
       "3            0.0         512.0          12.0         0.0               0.0   \n",
       "4            0.0         512.0          12.0         0.0               0.0   \n",
       "\n",
       "   reloc_writable  ndata_r_size  ndata_v_size  ndata_code  ndata_executable  \\\n",
       "0             0.0           0.0           0.0         0.0               0.0   \n",
       "1             0.0           0.0           0.0         0.0               0.0   \n",
       "2             0.0           0.0           0.0         0.0               0.0   \n",
       "3             0.0           0.0           0.0         0.0               0.0   \n",
       "4             0.0           0.0           0.0         0.0               0.0   \n",
       "\n",
       "   ndata_writable  rdatap_r_size  rdatap_v_size  rdatap_code  \\\n",
       "0             0.0            0.0            0.0          0.0   \n",
       "1             0.0            0.0            0.0          0.0   \n",
       "2             0.0            0.0            0.0          0.0   \n",
       "3             0.0            0.0            0.0          0.0   \n",
       "4             0.0            0.0            0.0          0.0   \n",
       "\n",
       "   rdatap_executable  rdatap_writable  brdata_r_size  brdata_v_size  \\\n",
       "0                0.0              0.0            0.0            0.0   \n",
       "1                0.0              0.0            0.0            0.0   \n",
       "2                0.0              0.0            0.0            0.0   \n",
       "3                0.0              0.0            0.0            0.0   \n",
       "4                0.0              0.0            0.0            0.0   \n",
       "\n",
       "   brdata_code  brdata_executable  brdata_writable  upx0_r_size  upx0_v_size  \\\n",
       "0          0.0                0.0              0.0          0.0          0.0   \n",
       "1          0.0                0.0              0.0          0.0          0.0   \n",
       "2          0.0                0.0              0.0          0.0          0.0   \n",
       "3          0.0                0.0              0.0          0.0          0.0   \n",
       "4          0.0                0.0              0.0          0.0          0.0   \n",
       "\n",
       "   upx0_code  upx0_executable  upx0_writable  upx1_r_size  upx1_v_size  \\\n",
       "0        0.0              0.0            0.0          0.0          0.0   \n",
       "1        0.0              0.0            0.0          0.0          0.0   \n",
       "2        0.0              0.0            0.0          0.0          0.0   \n",
       "3        0.0              0.0            0.0          0.0          0.0   \n",
       "4        0.0              0.0            0.0          0.0          0.0   \n",
       "\n",
       "   upx1_code  upx1_executable  upx1_writable  pdata_r_size  pdata_v_size  \\\n",
       "0        0.0              0.0            0.0           0.0           0.0   \n",
       "1        0.0              0.0            0.0           0.0           0.0   \n",
       "2        0.0              0.0            0.0           0.0           0.0   \n",
       "3        0.0              0.0            0.0           0.0           0.0   \n",
       "4        0.0              0.0            0.0           0.0           0.0   \n",
       "\n",
       "   pdata_code  pdata_executable  pdata_writable  tls_r_size  tls_v_size  \\\n",
       "0         0.0               0.0             0.0         0.0         0.0   \n",
       "1         0.0               0.0             0.0         0.0         0.0   \n",
       "2         0.0               0.0             0.0         0.0         0.0   \n",
       "3         0.0               0.0             0.0         0.0         0.0   \n",
       "4         0.0               0.0             0.0         0.0         0.0   \n",
       "\n",
       "   tls_code  tls_executable  tls_writable  idata_r_size  idata_v_size  \\\n",
       "0       0.0             0.0           0.0           0.0           0.0   \n",
       "1       0.0             0.0           0.0           0.0           0.0   \n",
       "2       0.0             0.0           0.0           0.0           0.0   \n",
       "3       0.0             0.0           0.0           0.0           0.0   \n",
       "4       0.0             0.0           0.0           0.0           0.0   \n",
       "\n",
       "   idata_code  idata_executable  idata_writable  crt_r_size  crt_v_size  \\\n",
       "0         0.0               0.0             0.0         0.0         0.0   \n",
       "1         0.0               0.0             0.0         0.0         0.0   \n",
       "2         0.0               0.0             0.0         0.0         0.0   \n",
       "3         0.0               0.0             0.0         0.0         0.0   \n",
       "4         0.0               0.0             0.0         0.0         0.0   \n",
       "\n",
       "   crt_code  crt_executable  crt_writable  upx2_r_size  upx2_v_size  \\\n",
       "0       0.0             0.0           0.0          0.0          0.0   \n",
       "1       0.0             0.0           0.0          0.0          0.0   \n",
       "2       0.0             0.0           0.0          0.0          0.0   \n",
       "3       0.0             0.0           0.0          0.0          0.0   \n",
       "4       0.0             0.0           0.0          0.0          0.0   \n",
       "\n",
       "   upx2_code  upx2_executable  upx2_writable  gfids_r_size  gfids_v_size  \\\n",
       "0        0.0              0.0            0.0           0.0           0.0   \n",
       "1        0.0              0.0            0.0           0.0           0.0   \n",
       "2        0.0              0.0            0.0           0.0           0.0   \n",
       "3        0.0              0.0            0.0           0.0           0.0   \n",
       "4        0.0              0.0            0.0           0.0           0.0   \n",
       "\n",
       "   gfids_code  gfids_executable  gfids_writable  code_r_size  code_v_size  \\\n",
       "0         0.0               0.0             0.0          0.0          0.0   \n",
       "1         0.0               0.0             0.0          0.0          0.0   \n",
       "2         0.0               0.0             0.0          0.0          0.0   \n",
       "3         0.0               0.0             0.0          0.0          0.0   \n",
       "4         0.0               0.0             0.0          0.0          0.0   \n",
       "\n",
       "   code_code  code_executable  code_writable  bss_r_size  bss_v_size  \\\n",
       "0        0.0              0.0            0.0         0.0         0.0   \n",
       "1        0.0              0.0            0.0         0.0         0.0   \n",
       "2        0.0              0.0            0.0         0.0         0.0   \n",
       "3        0.0              0.0            0.0         0.0         0.0   \n",
       "4        0.0              0.0            0.0         0.0         0.0   \n",
       "\n",
       "   bss_code  bss_executable  bss_writable  didat_r_size  didat_v_size  \\\n",
       "0       0.0             0.0           0.0           0.0           0.0   \n",
       "1       0.0             0.0           0.0           0.0           0.0   \n",
       "2       0.0             0.0           0.0           0.0           0.0   \n",
       "3       0.0             0.0           0.0           0.0           0.0   \n",
       "4       0.0             0.0           0.0           0.0           0.0   \n",
       "\n",
       "   didat_code  didat_executable  didat_writable  aspack_r_size  aspack_v_size  \\\n",
       "0         0.0               0.0             0.0            0.0            0.0   \n",
       "1         0.0               0.0             0.0            0.0            0.0   \n",
       "2         0.0               0.0             0.0            0.0            0.0   \n",
       "3         0.0               0.0             0.0            0.0            0.0   \n",
       "4         0.0               0.0             0.0            0.0            0.0   \n",
       "\n",
       "   aspack_code  aspack_executable  aspack_writable  xdata_r_size  \\\n",
       "0          0.0                0.0              0.0           0.0   \n",
       "1          0.0                0.0              0.0           0.0   \n",
       "2          0.0                0.0              0.0           0.0   \n",
       "3          0.0                0.0              0.0           0.0   \n",
       "4          0.0                0.0              0.0           0.0   \n",
       "\n",
       "   xdata_v_size  xdata_code  xdata_executable  xdata_writable  edata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   edata_v_size  edata_code  edata_executable  edata_writable  itext_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   itext_v_size  itext_code  itext_executable  itext_writable  adata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   adata_v_size  adata_code  adata_executable  adata_writable  didata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0            0.0   \n",
       "1           0.0         0.0               0.0             0.0            0.0   \n",
       "2           0.0         0.0               0.0             0.0            0.0   \n",
       "3           0.0         0.0               0.0             0.0            0.0   \n",
       "4           0.0         0.0               0.0             0.0            0.0   \n",
       "\n",
       "   didata_v_size  didata_code  didata_executable  didata_writable  \\\n",
       "0            0.0          0.0                0.0              0.0   \n",
       "1            0.0          0.0                0.0              0.0   \n",
       "2            0.0          0.0                0.0              0.0   \n",
       "3            0.0          0.0                0.0              0.0   \n",
       "4            0.0          0.0                0.0              0.0   \n",
       "\n",
       "   rmnet_r_size  rmnet_v_size  rmnet_code  rmnet_executable  rmnet_writable  \\\n",
       "0           0.0           0.0         0.0               0.0             0.0   \n",
       "1           0.0           0.0         0.0               0.0             0.0   \n",
       "2           0.0           0.0         0.0               0.0             0.0   \n",
       "3           0.0           0.0         0.0               0.0             0.0   \n",
       "4           0.0           0.0         0.0               0.0             0.0   \n",
       "\n",
       "   tsustub_r_size  tsustub_v_size  tsustub_code  tsustub_executable  \\\n",
       "0             0.0             0.0           0.0                 0.0   \n",
       "1             0.0             0.0           0.0                 0.0   \n",
       "2             0.0             0.0           0.0                 0.0   \n",
       "3             0.0             0.0           0.0                 0.0   \n",
       "4             0.0             0.0           0.0                 0.0   \n",
       "\n",
       "   tsustub_writable  tsuarch_r_size  tsuarch_v_size  tsuarch_code  \\\n",
       "0               0.0             0.0             0.0           0.0   \n",
       "1               0.0             0.0             0.0           0.0   \n",
       "2               0.0             0.0             0.0           0.0   \n",
       "3               0.0             0.0             0.0           0.0   \n",
       "4               0.0             0.0             0.0           0.0   \n",
       "\n",
       "   tsuarch_executable  tsuarch_writable  orpc_r_size  orpc_v_size  orpc_code  \\\n",
       "0                 0.0               0.0          0.0          0.0        0.0   \n",
       "1                 0.0               0.0          0.0          0.0        0.0   \n",
       "2                 0.0               0.0          0.0          0.0        0.0   \n",
       "3                 0.0               0.0          0.0          0.0        0.0   \n",
       "4                 0.0               0.0          0.0          0.0        0.0   \n",
       "\n",
       "   orpc_executable  orpc_writable  \n",
       "0              0.0            0.0  \n",
       "1              0.0            0.0  \n",
       "2              0.0            0.0  \n",
       "3              0.0            0.0  \n",
       "4              0.0            0.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some samples have no sections in the top percentile hence they have empyty dataframes,this column will be removed later\n",
    "for row,i in enumerate(df_list):\n",
    "    if i.empty:\n",
    "        df_list[row] = pd.DataFrame({\"EMPTY_PLACEHOLDER\": [0]})\n",
    "\n",
    "sections_df = pd.concat(df_list,ignore_index=True)\n",
    "sections_df = sections_df.fillna(0)\n",
    "sections_df = sections_df.drop(columns='EMPTY_PLACEHOLDER')\n",
    "sections_df.drop(columns=['_code', '_executable', '_writable', '_r_size','_v_size'], inplace=True)\n",
    "sections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_r_size</th>\n",
       "      <th>text_v_size</th>\n",
       "      <th>text_code</th>\n",
       "      <th>text_executable</th>\n",
       "      <th>text_writable</th>\n",
       "      <th>rdata_r_size</th>\n",
       "      <th>rdata_v_size</th>\n",
       "      <th>rdata_code</th>\n",
       "      <th>rdata_executable</th>\n",
       "      <th>rdata_writable</th>\n",
       "      <th>data_r_size</th>\n",
       "      <th>data_v_size</th>\n",
       "      <th>data_code</th>\n",
       "      <th>data_executable</th>\n",
       "      <th>data_writable</th>\n",
       "      <th>sxdata_r_size</th>\n",
       "      <th>sxdata_v_size</th>\n",
       "      <th>sxdata_code</th>\n",
       "      <th>sxdata_executable</th>\n",
       "      <th>sxdata_writable</th>\n",
       "      <th>rsrc_r_size</th>\n",
       "      <th>rsrc_v_size</th>\n",
       "      <th>rsrc_code</th>\n",
       "      <th>rsrc_executable</th>\n",
       "      <th>rsrc_writable</th>\n",
       "      <th>reloc_r_size</th>\n",
       "      <th>reloc_v_size</th>\n",
       "      <th>reloc_code</th>\n",
       "      <th>reloc_executable</th>\n",
       "      <th>reloc_writable</th>\n",
       "      <th>ndata_r_size</th>\n",
       "      <th>ndata_v_size</th>\n",
       "      <th>ndata_code</th>\n",
       "      <th>ndata_executable</th>\n",
       "      <th>ndata_writable</th>\n",
       "      <th>rdatap_r_size</th>\n",
       "      <th>rdatap_v_size</th>\n",
       "      <th>rdatap_code</th>\n",
       "      <th>rdatap_executable</th>\n",
       "      <th>rdatap_writable</th>\n",
       "      <th>brdata_r_size</th>\n",
       "      <th>brdata_v_size</th>\n",
       "      <th>brdata_code</th>\n",
       "      <th>brdata_executable</th>\n",
       "      <th>brdata_writable</th>\n",
       "      <th>upx0_r_size</th>\n",
       "      <th>upx0_v_size</th>\n",
       "      <th>upx0_code</th>\n",
       "      <th>upx0_executable</th>\n",
       "      <th>upx0_writable</th>\n",
       "      <th>upx1_r_size</th>\n",
       "      <th>upx1_v_size</th>\n",
       "      <th>upx1_code</th>\n",
       "      <th>upx1_executable</th>\n",
       "      <th>upx1_writable</th>\n",
       "      <th>pdata_r_size</th>\n",
       "      <th>pdata_v_size</th>\n",
       "      <th>pdata_code</th>\n",
       "      <th>pdata_executable</th>\n",
       "      <th>pdata_writable</th>\n",
       "      <th>tls_r_size</th>\n",
       "      <th>tls_v_size</th>\n",
       "      <th>tls_code</th>\n",
       "      <th>tls_executable</th>\n",
       "      <th>tls_writable</th>\n",
       "      <th>idata_r_size</th>\n",
       "      <th>idata_v_size</th>\n",
       "      <th>idata_code</th>\n",
       "      <th>idata_executable</th>\n",
       "      <th>idata_writable</th>\n",
       "      <th>crt_r_size</th>\n",
       "      <th>crt_v_size</th>\n",
       "      <th>crt_code</th>\n",
       "      <th>crt_executable</th>\n",
       "      <th>crt_writable</th>\n",
       "      <th>upx2_r_size</th>\n",
       "      <th>upx2_v_size</th>\n",
       "      <th>upx2_code</th>\n",
       "      <th>upx2_executable</th>\n",
       "      <th>upx2_writable</th>\n",
       "      <th>gfids_r_size</th>\n",
       "      <th>gfids_v_size</th>\n",
       "      <th>gfids_code</th>\n",
       "      <th>gfids_executable</th>\n",
       "      <th>gfids_writable</th>\n",
       "      <th>code_r_size</th>\n",
       "      <th>code_v_size</th>\n",
       "      <th>code_code</th>\n",
       "      <th>code_executable</th>\n",
       "      <th>code_writable</th>\n",
       "      <th>bss_r_size</th>\n",
       "      <th>bss_v_size</th>\n",
       "      <th>bss_code</th>\n",
       "      <th>bss_executable</th>\n",
       "      <th>bss_writable</th>\n",
       "      <th>didat_r_size</th>\n",
       "      <th>didat_v_size</th>\n",
       "      <th>didat_code</th>\n",
       "      <th>didat_executable</th>\n",
       "      <th>didat_writable</th>\n",
       "      <th>aspack_r_size</th>\n",
       "      <th>aspack_v_size</th>\n",
       "      <th>aspack_code</th>\n",
       "      <th>aspack_executable</th>\n",
       "      <th>aspack_writable</th>\n",
       "      <th>xdata_r_size</th>\n",
       "      <th>xdata_v_size</th>\n",
       "      <th>xdata_code</th>\n",
       "      <th>xdata_executable</th>\n",
       "      <th>xdata_writable</th>\n",
       "      <th>edata_r_size</th>\n",
       "      <th>edata_v_size</th>\n",
       "      <th>edata_code</th>\n",
       "      <th>edata_executable</th>\n",
       "      <th>edata_writable</th>\n",
       "      <th>itext_r_size</th>\n",
       "      <th>itext_v_size</th>\n",
       "      <th>itext_code</th>\n",
       "      <th>itext_executable</th>\n",
       "      <th>itext_writable</th>\n",
       "      <th>adata_r_size</th>\n",
       "      <th>adata_v_size</th>\n",
       "      <th>adata_code</th>\n",
       "      <th>adata_executable</th>\n",
       "      <th>adata_writable</th>\n",
       "      <th>didata_r_size</th>\n",
       "      <th>didata_v_size</th>\n",
       "      <th>didata_code</th>\n",
       "      <th>didata_executable</th>\n",
       "      <th>didata_writable</th>\n",
       "      <th>rmnet_r_size</th>\n",
       "      <th>rmnet_v_size</th>\n",
       "      <th>rmnet_code</th>\n",
       "      <th>rmnet_executable</th>\n",
       "      <th>rmnet_writable</th>\n",
       "      <th>tsustub_r_size</th>\n",
       "      <th>tsustub_v_size</th>\n",
       "      <th>tsustub_code</th>\n",
       "      <th>tsustub_executable</th>\n",
       "      <th>tsustub_writable</th>\n",
       "      <th>tsuarch_r_size</th>\n",
       "      <th>tsuarch_v_size</th>\n",
       "      <th>tsuarch_code</th>\n",
       "      <th>tsuarch_executable</th>\n",
       "      <th>tsuarch_writable</th>\n",
       "      <th>orpc_r_size</th>\n",
       "      <th>orpc_v_size</th>\n",
       "      <th>orpc_code</th>\n",
       "      <th>orpc_executable</th>\n",
       "      <th>orpc_writable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>29696.0</td>\n",
       "      <td>29442.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>38006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>36828.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13312.0</td>\n",
       "      <td>12936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>196608.0</td>\n",
       "      <td>194161.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>262144.0</td>\n",
       "      <td>261943.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89600.0</td>\n",
       "      <td>89294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7680.0</td>\n",
       "      <td>11404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>749568.0</td>\n",
       "      <td>749176.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15872.0</td>\n",
       "      <td>15488.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>23552.0</td>\n",
       "      <td>23364.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  text_r_size  text_v_size  text_code  text_executable  text_writable  \\\n",
       "0      1      29696.0      29442.0        1.0              1.0            0.0   \n",
       "1      0     196608.0     194161.0        1.0              1.0            0.0   \n",
       "2      1     262144.0     261943.0        1.0              1.0            0.0   \n",
       "3      0      23552.0      23364.0        1.0              1.0            0.0   \n",
       "4      0       2048.0       1848.0        1.0              1.0            0.0   \n",
       "\n",
       "   rdata_r_size  rdata_v_size  rdata_code  rdata_executable  rdata_writable  \\\n",
       "0       38400.0       38006.0         0.0               0.0             0.0   \n",
       "1           0.0           0.0         0.0               0.0             0.0   \n",
       "2       89600.0       89294.0         0.0               0.0             0.0   \n",
       "3           0.0           0.0         0.0               0.0             0.0   \n",
       "4           0.0           0.0         0.0               0.0             0.0   \n",
       "\n",
       "   data_r_size  data_v_size  data_code  data_executable  data_writable  \\\n",
       "0       1024.0      36828.0        0.0              0.0            1.0   \n",
       "1          0.0          0.0        0.0              0.0            0.0   \n",
       "2       7680.0      11404.0        0.0              0.0            1.0   \n",
       "3          0.0          0.0        0.0              0.0            0.0   \n",
       "4          0.0          0.0        0.0              0.0            0.0   \n",
       "\n",
       "   sxdata_r_size  sxdata_v_size  sxdata_code  sxdata_executable  \\\n",
       "0          512.0            8.0          0.0                0.0   \n",
       "1            0.0            0.0          0.0                0.0   \n",
       "2            0.0            0.0          0.0                0.0   \n",
       "3            0.0            0.0          0.0                0.0   \n",
       "4            0.0            0.0          0.0                0.0   \n",
       "\n",
       "   sxdata_writable  rsrc_r_size  rsrc_v_size  rsrc_code  rsrc_executable  \\\n",
       "0              1.0      13312.0      12936.0        0.0              0.0   \n",
       "1              0.0       4096.0       1960.0        0.0              0.0   \n",
       "2              0.0     749568.0     749176.0        0.0              0.0   \n",
       "3              0.0       1536.0       1304.0        0.0              0.0   \n",
       "4              0.0       1536.0       1232.0        0.0              0.0   \n",
       "\n",
       "   rsrc_writable  reloc_r_size  reloc_v_size  reloc_code  reloc_executable  \\\n",
       "0            0.0        1536.0        1274.0         0.0               0.0   \n",
       "1            0.0           0.0           0.0         0.0               0.0   \n",
       "2            0.0       15872.0       15488.0         0.0               0.0   \n",
       "3            0.0         512.0          12.0         0.0               0.0   \n",
       "4            0.0         512.0          12.0         0.0               0.0   \n",
       "\n",
       "   reloc_writable  ndata_r_size  ndata_v_size  ndata_code  ndata_executable  \\\n",
       "0             0.0           0.0           0.0         0.0               0.0   \n",
       "1             0.0           0.0           0.0         0.0               0.0   \n",
       "2             0.0           0.0           0.0         0.0               0.0   \n",
       "3             0.0           0.0           0.0         0.0               0.0   \n",
       "4             0.0           0.0           0.0         0.0               0.0   \n",
       "\n",
       "   ndata_writable  rdatap_r_size  rdatap_v_size  rdatap_code  \\\n",
       "0             0.0            0.0            0.0          0.0   \n",
       "1             0.0            0.0            0.0          0.0   \n",
       "2             0.0            0.0            0.0          0.0   \n",
       "3             0.0            0.0            0.0          0.0   \n",
       "4             0.0            0.0            0.0          0.0   \n",
       "\n",
       "   rdatap_executable  rdatap_writable  brdata_r_size  brdata_v_size  \\\n",
       "0                0.0              0.0            0.0            0.0   \n",
       "1                0.0              0.0            0.0            0.0   \n",
       "2                0.0              0.0            0.0            0.0   \n",
       "3                0.0              0.0            0.0            0.0   \n",
       "4                0.0              0.0            0.0            0.0   \n",
       "\n",
       "   brdata_code  brdata_executable  brdata_writable  upx0_r_size  upx0_v_size  \\\n",
       "0          0.0                0.0              0.0          0.0          0.0   \n",
       "1          0.0                0.0              0.0          0.0          0.0   \n",
       "2          0.0                0.0              0.0          0.0          0.0   \n",
       "3          0.0                0.0              0.0          0.0          0.0   \n",
       "4          0.0                0.0              0.0          0.0          0.0   \n",
       "\n",
       "   upx0_code  upx0_executable  upx0_writable  upx1_r_size  upx1_v_size  \\\n",
       "0        0.0              0.0            0.0          0.0          0.0   \n",
       "1        0.0              0.0            0.0          0.0          0.0   \n",
       "2        0.0              0.0            0.0          0.0          0.0   \n",
       "3        0.0              0.0            0.0          0.0          0.0   \n",
       "4        0.0              0.0            0.0          0.0          0.0   \n",
       "\n",
       "   upx1_code  upx1_executable  upx1_writable  pdata_r_size  pdata_v_size  \\\n",
       "0        0.0              0.0            0.0           0.0           0.0   \n",
       "1        0.0              0.0            0.0           0.0           0.0   \n",
       "2        0.0              0.0            0.0           0.0           0.0   \n",
       "3        0.0              0.0            0.0           0.0           0.0   \n",
       "4        0.0              0.0            0.0           0.0           0.0   \n",
       "\n",
       "   pdata_code  pdata_executable  pdata_writable  tls_r_size  tls_v_size  \\\n",
       "0         0.0               0.0             0.0         0.0         0.0   \n",
       "1         0.0               0.0             0.0         0.0         0.0   \n",
       "2         0.0               0.0             0.0         0.0         0.0   \n",
       "3         0.0               0.0             0.0         0.0         0.0   \n",
       "4         0.0               0.0             0.0         0.0         0.0   \n",
       "\n",
       "   tls_code  tls_executable  tls_writable  idata_r_size  idata_v_size  \\\n",
       "0       0.0             0.0           0.0           0.0           0.0   \n",
       "1       0.0             0.0           0.0           0.0           0.0   \n",
       "2       0.0             0.0           0.0           0.0           0.0   \n",
       "3       0.0             0.0           0.0           0.0           0.0   \n",
       "4       0.0             0.0           0.0           0.0           0.0   \n",
       "\n",
       "   idata_code  idata_executable  idata_writable  crt_r_size  crt_v_size  \\\n",
       "0         0.0               0.0             0.0         0.0         0.0   \n",
       "1         0.0               0.0             0.0         0.0         0.0   \n",
       "2         0.0               0.0             0.0         0.0         0.0   \n",
       "3         0.0               0.0             0.0         0.0         0.0   \n",
       "4         0.0               0.0             0.0         0.0         0.0   \n",
       "\n",
       "   crt_code  crt_executable  crt_writable  upx2_r_size  upx2_v_size  \\\n",
       "0       0.0             0.0           0.0          0.0          0.0   \n",
       "1       0.0             0.0           0.0          0.0          0.0   \n",
       "2       0.0             0.0           0.0          0.0          0.0   \n",
       "3       0.0             0.0           0.0          0.0          0.0   \n",
       "4       0.0             0.0           0.0          0.0          0.0   \n",
       "\n",
       "   upx2_code  upx2_executable  upx2_writable  gfids_r_size  gfids_v_size  \\\n",
       "0        0.0              0.0            0.0           0.0           0.0   \n",
       "1        0.0              0.0            0.0           0.0           0.0   \n",
       "2        0.0              0.0            0.0           0.0           0.0   \n",
       "3        0.0              0.0            0.0           0.0           0.0   \n",
       "4        0.0              0.0            0.0           0.0           0.0   \n",
       "\n",
       "   gfids_code  gfids_executable  gfids_writable  code_r_size  code_v_size  \\\n",
       "0         0.0               0.0             0.0          0.0          0.0   \n",
       "1         0.0               0.0             0.0          0.0          0.0   \n",
       "2         0.0               0.0             0.0          0.0          0.0   \n",
       "3         0.0               0.0             0.0          0.0          0.0   \n",
       "4         0.0               0.0             0.0          0.0          0.0   \n",
       "\n",
       "   code_code  code_executable  code_writable  bss_r_size  bss_v_size  \\\n",
       "0        0.0              0.0            0.0         0.0         0.0   \n",
       "1        0.0              0.0            0.0         0.0         0.0   \n",
       "2        0.0              0.0            0.0         0.0         0.0   \n",
       "3        0.0              0.0            0.0         0.0         0.0   \n",
       "4        0.0              0.0            0.0         0.0         0.0   \n",
       "\n",
       "   bss_code  bss_executable  bss_writable  didat_r_size  didat_v_size  \\\n",
       "0       0.0             0.0           0.0           0.0           0.0   \n",
       "1       0.0             0.0           0.0           0.0           0.0   \n",
       "2       0.0             0.0           0.0           0.0           0.0   \n",
       "3       0.0             0.0           0.0           0.0           0.0   \n",
       "4       0.0             0.0           0.0           0.0           0.0   \n",
       "\n",
       "   didat_code  didat_executable  didat_writable  aspack_r_size  aspack_v_size  \\\n",
       "0         0.0               0.0             0.0            0.0            0.0   \n",
       "1         0.0               0.0             0.0            0.0            0.0   \n",
       "2         0.0               0.0             0.0            0.0            0.0   \n",
       "3         0.0               0.0             0.0            0.0            0.0   \n",
       "4         0.0               0.0             0.0            0.0            0.0   \n",
       "\n",
       "   aspack_code  aspack_executable  aspack_writable  xdata_r_size  \\\n",
       "0          0.0                0.0              0.0           0.0   \n",
       "1          0.0                0.0              0.0           0.0   \n",
       "2          0.0                0.0              0.0           0.0   \n",
       "3          0.0                0.0              0.0           0.0   \n",
       "4          0.0                0.0              0.0           0.0   \n",
       "\n",
       "   xdata_v_size  xdata_code  xdata_executable  xdata_writable  edata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   edata_v_size  edata_code  edata_executable  edata_writable  itext_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   itext_v_size  itext_code  itext_executable  itext_writable  adata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0           0.0   \n",
       "1           0.0         0.0               0.0             0.0           0.0   \n",
       "2           0.0         0.0               0.0             0.0           0.0   \n",
       "3           0.0         0.0               0.0             0.0           0.0   \n",
       "4           0.0         0.0               0.0             0.0           0.0   \n",
       "\n",
       "   adata_v_size  adata_code  adata_executable  adata_writable  didata_r_size  \\\n",
       "0           0.0         0.0               0.0             0.0            0.0   \n",
       "1           0.0         0.0               0.0             0.0            0.0   \n",
       "2           0.0         0.0               0.0             0.0            0.0   \n",
       "3           0.0         0.0               0.0             0.0            0.0   \n",
       "4           0.0         0.0               0.0             0.0            0.0   \n",
       "\n",
       "   didata_v_size  didata_code  didata_executable  didata_writable  \\\n",
       "0            0.0          0.0                0.0              0.0   \n",
       "1            0.0          0.0                0.0              0.0   \n",
       "2            0.0          0.0                0.0              0.0   \n",
       "3            0.0          0.0                0.0              0.0   \n",
       "4            0.0          0.0                0.0              0.0   \n",
       "\n",
       "   rmnet_r_size  rmnet_v_size  rmnet_code  rmnet_executable  rmnet_writable  \\\n",
       "0           0.0           0.0         0.0               0.0             0.0   \n",
       "1           0.0           0.0         0.0               0.0             0.0   \n",
       "2           0.0           0.0         0.0               0.0             0.0   \n",
       "3           0.0           0.0         0.0               0.0             0.0   \n",
       "4           0.0           0.0         0.0               0.0             0.0   \n",
       "\n",
       "   tsustub_r_size  tsustub_v_size  tsustub_code  tsustub_executable  \\\n",
       "0             0.0             0.0           0.0                 0.0   \n",
       "1             0.0             0.0           0.0                 0.0   \n",
       "2             0.0             0.0           0.0                 0.0   \n",
       "3             0.0             0.0           0.0                 0.0   \n",
       "4             0.0             0.0           0.0                 0.0   \n",
       "\n",
       "   tsustub_writable  tsuarch_r_size  tsuarch_v_size  tsuarch_code  \\\n",
       "0               0.0             0.0             0.0           0.0   \n",
       "1               0.0             0.0             0.0           0.0   \n",
       "2               0.0             0.0             0.0           0.0   \n",
       "3               0.0             0.0             0.0           0.0   \n",
       "4               0.0             0.0             0.0           0.0   \n",
       "\n",
       "   tsuarch_executable  tsuarch_writable  orpc_r_size  orpc_v_size  orpc_code  \\\n",
       "0                 0.0               0.0          0.0          0.0        0.0   \n",
       "1                 0.0               0.0          0.0          0.0        0.0   \n",
       "2                 0.0               0.0          0.0          0.0        0.0   \n",
       "3                 0.0               0.0          0.0          0.0        0.0   \n",
       "4                 0.0               0.0          0.0          0.0        0.0   \n",
       "\n",
       "   orpc_executable  orpc_writable  \n",
       "0              0.0            0.0  \n",
       "1              0.0            0.0  \n",
       "2              0.0            0.0  \n",
       "3              0.0            0.0  \n",
       "4              0.0            0.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sect = sections_df.copy()\n",
    "sect = pd.concat([df['label'], sect], axis=1)\n",
    "sect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74318 training examples\n",
      "9290 validation examples\n",
      "9290 test examples\n"
     ]
    }
   ],
   "source": [
    "train, val, test = np.split(sect.sample(frac=1), [int(0.8*len(sect)), int(0.9*len(sect))])\n",
    "\n",
    "print(len(train), 'training examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/1785328546.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['label', 'text_r_size', 'text_v_size', 'text_code', 'text_executable', 'text_writable', 'rdata_r_size', 'rdata_v_size', 'rdata_code', 'rdata_executable', 'rdata_writable', 'data_r_size', 'data_v_size', 'data_code', 'data_executable', 'data_writable', 'sxdata_r_size', 'sxdata_v_size', 'sxdata_code', 'sxdata_executable', 'sxdata_writable', 'rsrc_r_size', 'rsrc_v_size', 'rsrc_code', 'rsrc_executable', 'rsrc_writable', 'reloc_r_size', 'reloc_v_size', 'reloc_code', 'reloc_executable', 'reloc_writable', 'ndata_r_size', 'ndata_v_size', 'ndata_code', 'ndata_executable', 'ndata_writable', 'rdatap_r_size', 'rdatap_v_size', 'rdatap_code', 'rdatap_executable', 'rdatap_writable', 'brdata_r_size', 'brdata_v_size', 'brdata_code', 'brdata_executable', 'brdata_writable', 'upx0_r_size', 'upx0_v_size', 'upx0_code', 'upx0_executable', 'upx0_writable', 'upx1_r_size', 'upx1_v_size', 'upx1_code', 'upx1_executable', 'upx1_writable', 'pdata_r_size', 'pdata_v_size', 'pdata_code', 'pdata_executable', 'pdata_writable', 'tls_r_size', 'tls_v_size', 'tls_code', 'tls_executable', 'tls_writable', 'idata_r_size', 'idata_v_size', 'idata_code', 'idata_executable', 'idata_writable', 'crt_r_size', 'crt_v_size', 'crt_code', 'crt_executable', 'crt_writable', 'upx2_r_size', 'upx2_v_size', 'upx2_code', 'upx2_executable', 'upx2_writable', 'gfids_r_size', 'gfids_v_size', 'gfids_code', 'gfids_executable', 'gfids_writable', 'code_r_size', 'code_v_size', 'code_code', 'code_executable', 'code_writable', 'bss_r_size', 'bss_v_size', 'bss_code', 'bss_executable', 'bss_writable', 'didat_r_size', 'didat_v_size', 'didat_code', 'didat_executable', 'didat_writable', 'aspack_r_size', 'aspack_v_size', 'aspack_code', 'aspack_executable', 'aspack_writable', 'xdata_r_size', 'xdata_v_size', 'xdata_code', 'xdata_executable', 'xdata_writable', 'edata_r_size', 'edata_v_size', 'edata_code', 'edata_executable', 'edata_writable', 'itext_r_size', 'itext_v_size', 'itext_code', 'itext_executable', 'itext_writable', 'adata_r_size', 'adata_v_size', 'adata_code', 'adata_executable', 'adata_writable', 'didata_r_size', 'didata_v_size', 'didata_code', 'didata_executable', 'didata_writable', 'rmnet_r_size', 'rmnet_v_size', 'rmnet_code', 'rmnet_executable', 'rmnet_writable', 'tsustub_r_size', 'tsustub_v_size', 'tsustub_code', 'tsustub_executable', 'tsustub_writable', 'tsuarch_r_size', 'tsuarch_v_size', 'tsuarch_code', 'tsuarch_executable', 'tsuarch_writable', 'orpc_r_size', 'orpc_v_size', 'orpc_code', 'orpc_executable', 'orpc_writable']\n",
      "A batch of targets: tf.Tensor([1 0 0 0 1], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('label')\n",
    "    df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a layer that turns strings into integer indices.\n",
    "    if dtype == 'string':\n",
    "        index = layers.StringLookup(max_tokens=max_tokens)\n",
    "    # Otherwise, create a layer that turns integer values into integer indices.\n",
    "    else:\n",
    "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Encode the integer indices.\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "    # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 92898), dtype=float32, numpy=\n",
       "array([[-0.0164463 , -0.01331942, -0.01209168, ..., -0.01495959,\n",
       "        -0.0165614 , -0.01631202]], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_count_col = sect['text_r_size']\n",
    "layer = get_normalization_layer('text_r_size', train_ds)\n",
    "layer(photo_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_age_col = sect['text_code']\n",
    "test_age_layer = get_category_encoding_layer(name='text_code',\n",
    "                                             dataset=train_ds,\n",
    "                                             dtype='float',\n",
    "                                             max_tokens=5)\n",
    "test_age_layer(test_age_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = sect.columns\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "for col in columns:\n",
    "    if col.endswith('size'):\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=col)\n",
    "        normalization_layer = get_normalization_layer(col, train_ds)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "\n",
    "    if col.endswith('code') or col.endswith('writable') or col.endswith('executable'):\n",
    "        categorical_col = tf.keras.Input(\n",
    "            shape=(1,), \n",
    "            name=col, \n",
    "            dtype='float'\n",
    "        )\n",
    "        encoding_layer = get_category_encoding_layer(\n",
    "            name=col,\n",
    "            dataset=train_ds,\n",
    "            dtype='float',\n",
    "            max_tokens=2\n",
    "        )\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        all_inputs.append(categorical_col)\n",
    "        encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(\n",
    "    256,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_1'\n",
    ")(all_features)\n",
    "#x = layers.Dropout(0.1, name='Dropout')(x)\n",
    "x = layers.Dense(\n",
    "    128,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_2'\n",
    ")(x)\n",
    "x = layers.Dense(\n",
    "    64,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_3'\n",
    ")(x)\n",
    "x = layers.Dense(\n",
    "    32,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_4'\n",
    ")(x)\n",
    "output = layers.Dense(\n",
    "    1,\n",
    "    activation='sigmoid',\n",
    "    name='Sigmoid'\n",
    ")(x)\n",
    "#output = layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_threshold = 0.6\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(\n",
    "            name='accuracy', \n",
    "            threshold=classification_threshold\n",
    "        ),\n",
    "        tf.keras.metrics.Precision(\n",
    "            name='precision',\n",
    "            thresholds=classification_threshold\n",
    "        ),\n",
    "        tf.keras.metrics.Recall(\n",
    "            name=\"recall\",\n",
    "            thresholds=classification_threshold\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/1785328546.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py:559: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162/1162 [==============================] - 120s 100ms/step - loss: 0.8607 - accuracy: 0.7582 - precision: 0.8645 - recall: 0.6813 - val_loss: 0.5257 - val_accuracy: 0.7665 - val_precision: 0.9597 - val_recall: 0.6106\n",
      "Epoch 2/5\n",
      "1162/1162 [==============================] - 121s 104ms/step - loss: 0.5041 - accuracy: 0.7806 - precision: 0.8787 - recall: 0.7124 - val_loss: 0.4762 - val_accuracy: 0.7858 - val_precision: 0.9494 - val_recall: 0.6540\n",
      "Epoch 3/5\n",
      "1162/1162 [==============================] - 122s 105ms/step - loss: 0.4725 - accuracy: 0.7894 - precision: 0.8841 - recall: 0.7245 - val_loss: 0.4539 - val_accuracy: 0.7879 - val_precision: 0.9502 - val_recall: 0.6575\n",
      "Epoch 4/5\n",
      "1162/1162 [==============================] - 121s 104ms/step - loss: 0.4558 - accuracy: 0.7968 - precision: 0.8898 - recall: 0.7333 - val_loss: 0.4407 - val_accuracy: 0.7906 - val_precision: 0.9503 - val_recall: 0.6625\n",
      "Epoch 5/5\n",
      "1162/1162 [==============================] - 125s 107ms/step - loss: 0.4450 - accuracy: 0.8013 - precision: 0.8940 - recall: 0.7379 - val_loss: 0.4321 - val_accuracy: 0.7927 - val_precision: 0.9476 - val_recall: 0.6684\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=5, \n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "epochs = history.epoch\n",
    "\n",
    "hist = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "Plot_Curve(epochs, hist, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/1785328546.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "C:\\Users\\blake\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py:559: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=batch_size)\n",
    "sections_pred = model.predict(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                43521\n",
       "label                                                                    1\n",
       "file_type                PE32 executable (GUI) Intel 80386 Mono/.Net as...\n",
       "file_size                                                         0.003819\n",
       "file_entropy                                                      0.790723\n",
       "imports                                                        _CorExeMain\n",
       "exports                                                                 []\n",
       "size_of_image                                                     0.000717\n",
       "size_of_code                                                      0.000112\n",
       "size_of_stack_reserve                                             0.001143\n",
       "size_of_stack_commit                                              0.000004\n",
       "size_of_heap_reserve                                              0.000538\n",
       "size_of_heap_commit                                               0.000003\n",
       "number_of_sections                                                0.041667\n",
       "sections                 [['.text', 480768, 480452, 1, 1, 0], ['.rsrc',...\n",
       "size_dif                                                          0.145623\n",
       "stack_dif                                                         0.565319\n",
       "heap_dif                                                           0.02229\n",
       "Name: 29638, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covert the data to dataframes. \n",
    "final_df = pd.concat(\n",
    "    [\n",
    "        df[['label', 'file_entropy']],\n",
    "        pd.DataFrame(imports_pred, columns=['imports']),\n",
    "        pd.DataFrame(type_pred, columns=['type']),\n",
    "        pd.DataFrame(sections_pred, columns=['sections'])\n",
    "    ], axis=1\n",
    ")\n",
    "final_df.rename(columns={'file_entropy':'entropy'}, inplace=True)\n",
    "final_df = final_df.iloc[:50000]\n",
    "final_df.sample(10)\n",
    "df.loc[29638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['entropy', 'imports', 'type', 'sections']\n",
    "feature_columns = []\n",
    "\n",
    "for col in columns:\n",
    "    feature_columns.append(feature_column.numeric_column(col))\n",
    "\n",
    "feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(feature_layer)\n",
    "model.add(layers.Dense(\n",
    "    64,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_1'\n",
    "))\n",
    "model.add(layers.Dense(\n",
    "    128,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_2'\n",
    "))\n",
    "\n",
    "model.add(layers.Dense(\n",
    "    64,\n",
    "    activation='relu',\n",
    "    kernel_regularizer='l2',\n",
    "    name='Hidden_3'\n",
    "))\n",
    "\n",
    "model.add(layers.Dropout(0.1, name='Dropout'))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(\n",
    "    1,\n",
    "    activation='sigmoid',\n",
    "    name='Sigmoid'\n",
    "))\n",
    "\n",
    "\n",
    "classification_threshold = 0.52\n",
    "learning_rate = 0.003\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(\n",
    "            name='accuracy', \n",
    "            threshold=classification_threshold\n",
    "        ),\n",
    "        tf.keras.metrics.Precision(\n",
    "            name='precision',\n",
    "            thresholds=classification_threshold\n",
    "        ),\n",
    "        tf.keras.metrics.Recall(\n",
    "            name=\"recall\",\n",
    "            thresholds=classification_threshold\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\AppData\\Local\\Temp/ipykernel_21168/1785328546.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.7024 - accuracy: 0.5457 - precision: 0.5672 - recall: 0.8313WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'label': <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, 'entropy': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float64>, 'imports': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, 'type': <tf.Tensor 'IteratorGetNext:4' shape=(None, 1) dtype=float32>, 'sections': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.7024 - accuracy: 0.5458 - precision: 0.5673 - recall: 0.8314 - val_loss: 0.6820 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6847 - accuracy: 0.5465 - precision: 0.5652 - recall: 0.8608 - val_loss: 0.6846 - val_accuracy: 0.4251 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6847 - accuracy: 0.5480 - precision: 0.5659 - recall: 0.8634 - val_loss: 0.6820 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6847 - accuracy: 0.5617 - precision: 0.5663 - recall: 0.9617 - val_loss: 0.6821 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5608 - precision: 0.5661 - recall: 0.9588 - val_loss: 0.6821 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5658 - precision: 0.5658 - recall: 1.0000 - val_loss: 0.6825 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5658 - precision: 0.5658 - recall: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5658 - precision: 0.5658 - recall: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5658 - precision: 0.5658 - recall: 1.0000 - val_loss: 0.6820 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6846 - accuracy: 0.5630 - precision: 0.5656 - recall: 0.9816 - val_loss: 0.6819 - val_accuracy: 0.5749 - val_precision: 0.5749 - val_recall: 1.0000\n",
      "Epoch 11/50\n",
      "1527/2000 [=====================>........] - ETA: 3s - loss: 0.6853 - accuracy: 0.5599 - precision: 0.5628 - recall: 0.9775"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train, test = train_test_split(final_df, test_size=0.2, random_state=42)\n",
    "train, valid = train_test_split(train, test_size=0.2)\n",
    "\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(valid, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, batch_size=batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "   train_ds, \n",
    "   epochs=50, \n",
    "   validation_data=val_ds\n",
    ")\n",
    "\n",
    "epochs = history.epoch\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "#pred = model.predict(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "Plot_Curve(epochs, hist, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37847328], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[29638]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
